[["index.html", "Data Science portfolio Thijmen Weijgertze Main Page", " Data Science portfolio Thijmen Weijgertze Main Page Introduction This github page is written by Thijmen Weijgertze using Rstudio. I am a 3rd year Life Science student at HU University of Applied Sciences Utrecht. In the past half year I’ve followed a DataScience minor provided by the HU. This Github page serves as my DataScience portofolio for showing off my DataScience skills. The course is created by @amdhaanTlscdsfb26v20Workflows2021 based on the “bookdown: Authoring Books and Technical Documents with R Markdown” book from @xieBookdownAuthoringBooks. Contact information E-mail GitHub LinkedIn "],["metadata.html", "MetaData", " MetaData Source code Bookdown source code: PortofolioBookdown Website source code: ThijmenWeijgertze-portofolio Website link: thijmenweijgertze.github.io/ThijmenWeijgertze-portfolio/ Dependency packages tidyverse tidymodels bookdown reactable gt fs tidyverse RColorBrewer readxl ggfortify twPackage (devtools::install_github(“ThijmenWeijgertze/twPackage”, build_vignettes = TRUE)) Directory roadmap The structure of this portfolio’s source code is as follows: library(fs) library(tidyverse) dir_tree(&quot;.&quot;, recurse = TRUE, regexp = &quot;^.gitignore$|^[^_|(.git)]&quot;, all = TRUE) ## . ## ├── .gitignore ## ├── 00-METADATA.Rmd ## ├── 01_CV.Rmd ## ├── 02-GuerrillaAnalytics.Rmd ## ├── 03-CElegans.Rmd ## ├── 04-ReproducibleResearch.Rmd ## ├── 05-MachineLearning.Rmd ## ├── 06-Package.Rmd ## ├── 07-DataScienceProject.Rmd ## ├── 09-CovidCases.Rmd ## ├── CleanBook.R ## ├── css ## │ └── style.css ## ├── data ## │ ├── breast-cancer.data ## │ ├── breast-cancer.names ## │ ├── CE-LIQ-FLOW-062_Tidydata.xlsx ## │ ├── CovidData.csv ## │ └── GuerrillaAnalytics.png ## ├── dataRaw ## │ ├── breast-cancer.data ## │ ├── breast-cancer.names ## │ ├── CE.LIQ.FLOW.062_Tidydata.xlsx ## │ ├── data.csv ## │ ├── GuerrillaAnalytics.png ## │ └── README.dataRaw.Rmd ## ├── doc ## │ ├── github.placeholder ## │ ├── notes.txt ## │ └── README.doc.Rmd ## ├── LICENSE ## ├── PortofolioBookdown.Rproj ## └── README.md dataRaw &amp; data folder The dataRaw folder holds the original data files. These data files are unedited and remain in untouched in the folder as back-up. The data within the data folder are used within the Rmarkdown files and may differ from the original source in dataRaw. The data folder consists of the following files: - CE-LIQ-FLOW-062_Tidydata.xlsx: C.elegans data used in the C.elegans plate analysis - GuerrillaAnalytics.png: a screenshot of a project’s directory tree to demonstrate the guerrilla analytics principles "],["thijmen-weijgertze-datascience-cv.html", "Thijmen Weijgertze DataScience CV", " Thijmen Weijgertze DataScience CV About me I’m “Thijmen Weijgertze” (2003) from Ede, Gelderland. I tend to be very passionate about my study and love to talk about topics surrounding Life Sciences. As of writing this résumé I live with my parents, sister and our dog “Saartje” (breed: markiesje). In my free time my biggest hobby is music. I produce music from time to time and play piano almost daily. Besides music I also like to spent time with friends in the weekends and cycle so now and then. My education My most relevant skills related to DataScience (alphabetical order) Assembly tools I’ve worked with the assembly tools Unicycler, PlasmidEC and Gplas. These tools I’ve used within a project where we (our project group) assembled plasmids from bacterial illumina NGS data. This project was commissioned by the RIVM. Bash I’m capable of writing in Bash Cell biology I’ve theoretical knowledge about the principles of the cell such as cell metabolism and cellular communication Cell culture I’m capable of working and running experiments with cell cultures Git/Github I’ve worked with git/github workflows Immunology I’ve theoretical knowledge about the principles regarding immunology Metagenomics I’m capable of writing a pipeline for downstream metagenomics data NGS I’ve theoretical knowledge about the NGS principles of Pac-bio, Illumina and Nanopore Oncology I’ve theoretical knowledge about the hallmarks of tumorcells PCR I’m experienced with PCR both practicly and theoreticly Reproducibility I’m experienced regarding reproducible research R I’m capable of writing in R and experienced with rmarkdown. See my DataScience portofolio RNA/DNA isolation I’m capable of isolating RNA/DNA (including creating a cDNA bank) RNA-seq I’m capable of writing a pipeline for downstream rna-seq analysis using DESeq2 (with results such as heatmaps, count plots, volcano plots, up/downregulated genes Statistics I’m capable of parametric and non-parametric tests and writing a conclusion based on the results Contact info www: https://thijmenweijgertze.github.io/ThijmenWeijgertze-portfolio/ email: thijmen.lifesciences@gmail.com github: https://github.com/ThijmenWeijgertze linkedin: https://www.linkedin.com/in/thijmen-weijgertze-968a5a265/ "],["guerrilla-analytics.html", "Guerrilla Analytics", " Guerrilla Analytics Example Project # Load example image library(here) knitr::include_graphics(here::here( &quot;data&quot;, &quot;GuerrillaAnalytics.png&quot; )) Root Of This Bookdown Project library(fs) library(tidyverse) dir_tree(&quot;.&quot;, recurse = TRUE, regexp = &quot;^.gitignore$|^[^_|(.git)]&quot;, all = TRUE) ## . ## ├── .gitignore ## ├── 00-METADATA.Rmd ## ├── 01_CV.Rmd ## ├── 02-GuerrillaAnalytics.Rmd ## ├── 03-CElegans.Rmd ## ├── 04-ReproducibleResearch.Rmd ## ├── 05-MachineLearning.Rmd ## ├── 06-Package.Rmd ## ├── 07-DataScienceProject.Rmd ## ├── 09-CovidCases.Rmd ## ├── CleanBook.R ## ├── css ## │ └── style.css ## ├── data ## │ ├── breast-cancer.data ## │ ├── breast-cancer.names ## │ ├── CE-LIQ-FLOW-062_Tidydata.xlsx ## │ ├── CovidData.csv ## │ └── GuerrillaAnalytics.png ## ├── dataRaw ## │ ├── breast-cancer.data ## │ ├── breast-cancer.names ## │ ├── CE.LIQ.FLOW.062_Tidydata.xlsx ## │ ├── data.csv ## │ ├── GuerrillaAnalytics.png ## │ └── README.dataRaw.Rmd ## ├── doc ## │ ├── github.placeholder ## │ ├── notes.txt ## │ └── README.doc.Rmd ## ├── LICENSE ## ├── PortofolioBookdown.Rproj ## └── README.md "],["c.-elegans-plate-experiment.html", "C. Elegans plate experiment", " C. Elegans plate experiment Setup Setting a seed and loading packages # Seed chosen based on the current year set.seed(2023) # Loading packages library(tidyverse) library(RColorBrewer) library(readxl) library(here) library(reactable) Importing and inspecting the data According to The data was kindly supplied by J. Louter (INT/ILC) and was derived from an experiment in which adult C.elegans nematodes were exposed to varying concentrations of different compounds. # importing xlsx file CE_LIQ_FLOW_062_Tidydata &lt;- read_excel( here::here( &quot;data&quot;, &quot;CE-LIQ-FLOW-062_Tidydata.xlsx&quot; ) ) # inspecting data in table format with the reactable package reactable(CE_LIQ_FLOW_062_Tidydata, defaultPageSize = 5, compact = TRUE) Scatterplot Pseudocode Deciding which columns will be included in the scatterplot Checking and possibly changing the datatypes of those columns Plotting the data in a scatterplot using ggplot Normalizing y-axis counts Setting the x-axis to a log10 scale Adding jitter to spread out points on top of eachother Checking and correcting datatypes needed for the scatterplot # Checking the data types of the following columns: RawData, compName, expType and compConcentration CE_LIQ_FLOW_062_Tidydata %&gt;% select(RawData, compName, compConcentration, expType) %&gt;% str() ## tibble [360 × 4] (S3: tbl_df/tbl/data.frame) ## $ RawData : num [1:360] 44 37 45 47 41 35 41 36 40 38 ... ## $ compName : chr [1:360] &quot;2,6-diisopropylnaphthalene&quot; &quot;2,6-diisopropylnaphthalene&quot; &quot;2,6-diisopropylnaphthalene&quot; &quot;2,6-diisopropylnaphthalene&quot; ... ## $ compConcentration: chr [1:360] &quot;4.99&quot; &quot;4.99&quot; &quot;4.99&quot; &quot;4.99&quot; ... ## $ expType : chr [1:360] &quot;experiment&quot; &quot;experiment&quot; &quot;experiment&quot; &quot;experiment&quot; ... # Changing compConcentration to numeric; changing compName into a factor; changing expType into a factor CE_LIQ_FLOW_062_Tidydata$compConcentration &lt;- parse_number(CE_LIQ_FLOW_062_Tidydata$compConcentration) CE_LIQ_FLOW_062_Tidydata$compName &lt;- factor(CE_LIQ_FLOW_062_Tidydata$compName, levels = unique(CE_LIQ_FLOW_062_Tidydata$compName)) CE_LIQ_FLOW_062_Tidydata$expType &lt;- factor(CE_LIQ_FLOW_062_Tidydata$expType, levels = unique(CE_LIQ_FLOW_062_Tidydata$expType)) # Checking the new dataypes and factor levels CE_LIQ_FLOW_062_Tidydata %&gt;% select(RawData, compName, compConcentration, expType) %&gt;% str() ## tibble [360 × 4] (S3: tbl_df/tbl/data.frame) ## $ RawData : num [1:360] 44 37 45 47 41 35 41 36 40 38 ... ## $ compName : Factor w/ 5 levels &quot;2,6-diisopropylnaphthalene&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... ## $ compConcentration: num [1:360] 4.99 4.99 4.99 4.99 4.99 4.99 4.99 4.99 4.99 4.99 ... ## $ expType : Factor w/ 4 levels &quot;experiment&quot;,&quot;controlPositive&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... levels(CE_LIQ_FLOW_062_Tidydata$compName) ## [1] &quot;2,6-diisopropylnaphthalene&quot; &quot;decane&quot; ## [3] &quot;naphthalene&quot; &quot;Ethanol&quot; ## [5] &quot;S-medium&quot; levels(CE_LIQ_FLOW_062_Tidydata$expType) ## [1] &quot;experiment&quot; &quot;controlPositive&quot; &quot;controlNegative&quot; &quot;controlVehicleA&quot; normalizing data CE_LIQ_FLOW_062_contNeg &lt;- CE_LIQ_FLOW_062_Tidydata %&gt;% filter(expType == &quot;controlNegative&quot;) contNeg_mean &lt;- mean(CE_LIQ_FLOW_062_contNeg$RawData) contNeg_mean ## [1] 85.9 CE_LIQ_FLOW_062_Tidydata &lt;- CE_LIQ_FLOW_062_Tidydata %&gt;% mutate(normalized = RawData/mean(contNeg_mean)) mean(CE_LIQ_FLOW_062_Tidydata$RawData) ## [1] NA CE_LIQ_FLOW_062_Tidydata %&gt;% select(compName, expType, compConcentration, RawData, normalized) %&gt;% reactable(defaultPageSize = 5) CE_LIQ_FLOW_062_contNeg &lt;- CE_LIQ_FLOW_062_Tidydata %&gt;% filter(expType == &quot;controlNegative&quot;) contNegNorm_mean &lt;- mean(CE_LIQ_FLOW_062_contNeg$normalized) contNegNorm_mean ## [1] 1 # https://www.statology.org/how-to-normalize-data-in-r/ Plotting the CE_LIQ_FLOW_062_normalized in a scatterplot # Plotting the CE_LIQ_FLOW_062_normalized in a scatterplot ggplot( data = CE_LIQ_FLOW_062_Tidydata, aes(x = log10(compConcentration), y = normalized))+ geom_point( aes(color = compName, shape = expType), size = 1.5, alpha = 0.8, position = position_jitter(width = 0.1))+ # jitter to spread out values on top of eachother labs( title = &quot;Scatterplot CE_LIQ_FLOW_062_Tidydata&quot;, caption = &quot;Data normalized with the negative control mean&quot;, x = &quot;log10(compConcentration) in nM&quot;, y = &quot;RawData in counts&quot;)+ scale_color_brewer(palette = &quot;Dark2&quot;) # colorblind friendly color scale Figure 1: The positive control for this experiments is controlPositive The negative control for this experiment is controlNegative Statistical tests Pseudocode Difference between the compound concentrations Loading the data Filter the concentrations per compound Plot the data using bargraph with the stdev as error bar Check the normality with a Shapio-Wilk test of both groups separately Perform an ANOVA test If the ANOVA test is significant, perform post hoc tests Draw a conclusion Difference between the LC50 curves Loading the data Filter the concentrations per compound Plotting the LC50 curves per compound Calculating 95% confidence intervals Checking the overlap between the 95% confidence intervals Draw conclusion "],["open-peer-review.html", "Open Peer Review", " Open Peer Review Introduction The COVID-19 pandemic has highlighted how open science and reproducible research can speed up scientific progress. Open science makes it possible for different research facilities to share their finding with the rest of the world for others to build on. Reproducible research is an important subject within the Open Science. Not only should science publication be available to everyone (after the research has been completed) without any publisher pay wall, but it’s also important that publications are reproducible and the conclusions can be verified. In this chapter I’ll peer review an article from bioRxiv. The article chosen to review is: Exploring Metabolic Anomalies in COVID-19 and Post-COVID-19: A Machine Learning Approach with Explainable Artificial Intelligence Criteria The open peer review will use criteria from the following publication: Reproducibility and reporting practices in COVID-19 preprint manuscripts Transparency Criteria Definition Response Type Study Purpose A concise statement in the introduction of the article, often in the last paragraph, that establishes the reason the research was conducted. Also called the study objective. Binary Data Availability Statement A statement, in an individual section offset from the main body of text, that explains how or if one can access a study’s data. The title of the section may vary, but it must explicitly mention data; it is therefore distinct from a supplementary materials section. Binary Data Location Where the article’s data can be accessed, either raw or processed. Found Value Study Location Author has stated in the methods section where the study took place or the data’s country/region of origin. Binary; Found Value Author Review The professionalism of the contact information that the author has provided in the manuscript. Found Value Ethics Statement A statement within the manuscript indicating any ethical concerns, including the presence of sensitive data. Binary Funding Statement A statement within the manuscript indicating whether or not the authors received funding for their research. Binary Code Availability Authors have shared access to the most updated code that they used in their study, including code used for analysis. Binary About the article The research article states in its abstract to have used techniques such as machine learning (ML), explainable artificial intelligence (XAI) and SHapley Additive exPlanations (SHAP) to analyze metabolic alterations in COVID-19 and Post-COVID-19 patients. The publication aims to uncover metabolic signatures and identify potential biomarkers for these conditions. It states to have analyzed 142 COVID-19, 48 Post-COVID-19 samples and 38 CONTROL patients, with 111 identified metabolites. After which the results have been compared to traditional methods such as PCA and PLS-DA. The comparison showed that the combined techniques of ML, XAI and SHAP outperformed the traditional methods and provides different insights in the metabolic basis of the disease’s progression and its aftermath. Open Peer Review Transparency Criteria Definition Response Type Study Purpose A concise statement in the introduction of the article, often in the last paragraph, that establishes the reason the research was conducted. Also called the study objective. yes Data Availability Statement A statement, in an individual section offset from the main body of text, that explains how or if one can access a study’s data. The title of the section may vary, but it must explicitly mention data; it is therefore distinct from a supplementary materials section. yes Data Location Where the article’s data can be accessed, either raw or processed. online Study Location Author has stated in the methods section where the study took place or the data’s country/region of origin. yes; Mendeley database Author Review The professionalism of the contact information that the author has provided in the manuscript. Found Value Ethics Statement A statement within the manuscript indicating any ethical concerns, including the presence of sensitive data. no Funding Statement A statement within the manuscript indicating whether or not the authors received funding for their research. yes Code Availability Authors have shared access to the most updated code that they used in their study, including code used for analysis. yes Conclusion DE CODE STAAT NU WEL ONLINE DUS DE CONCLUSIE VERANDEREN &lt;– !!! The article scores positive in most expects except the code availibity and ethics section. Although the article specified a github link in their publication the link redirects to a 404 error page. This probably indicates that the repository is either set to private, renamed or deleted. The last version of the published pdf at this moment is from April 17th 2024. Looking through the other public repositories it doesn’t seem to be renamed as of writing this chapter (2024-04-24). Thereby the article does not pass the open peer review. "],["machine-learning.html", "Machine Learning", " Machine Learning Introduction During the “Data Science for biology 2” course every student has the opportunity to learn a new skill for their portfolio. I have decided to introduce myself into “Machine Learning”. I haven chosen machine learning, because over the past decade it has become an important skill in DataScience. Implementing algorithms to predict medical outcomes, using available data sets, can be extremely important for early medical diagnoses. Furthermore machine learning’s high computational power makes it possible to analyze more factors and relations within the data at once. This can be highly beneficial while working with large data sets within the field of Life Sciences. Goal At the end of this course I want to have created an algorithm able to predict the recurrence of breast cancer using the breast-cancer data set. The algorithm will use the R package “tidymodels” for machine learning. Data This breast cancer domain was obtained from the University Medical Centre, Institute of Oncology, Ljubljana, Yugoslavia. Thanks go to M. Zwitter and M. Soklic for providing the data. The table consist of the following variables: Class: no-recurrence-events, recurrence-events age: 10-19, 20-29, 30-39, 40-49, 50-59, 60-69, 70-79, 80-89, 90-99. menopause: lt40, ge40, premeno. tumor-size: 0-4, 5-9, 10-14, 15-19, 20-24, 25-29, 30-34, 35-39, 40-44, 45-49, 50-54, 55-59. inv-nodes: 0-2, 3-5, 6-8, 9-11, 12-14, 15-17, 18-20, 21-23, 24-26, 27-29, 30-32, 33-35, 36-39. node-caps: yes, no. deg-malig: 1, 2, 3. breast: left, right. breast-quad: left-up, left-low, right-up, right-low, central. irradiat: yes, no. There are a total of 286 instances from which 201 no-recurrence-events and 85 recurrence-events. Planning I’ll spend 30 hours across 4 days on this skill across 2 weeks Week 1: - Day 1: Delving into the get started guide from tidymodels - Day 2: Delving into the get started guide from tidymodels Week 2: - Day 3: Apply the learned skills on the breast-cancer data set to predict the recurrence of breast cancer. - Day 4: Extension of past days Dataset library(gt) # loading in data bcData &lt;- read.csv( here::here( &quot;data&quot;, &quot;breast-cancer.data&quot; ), header = FALSE, # dataset does not contain column names col.names = c(&quot;class&quot;, &quot;age&quot;, &quot;menopause&quot;, &quot;tumor_size&quot;, &quot;inv_nodes&quot;, &quot;node_caps&quot;, &quot;deg_malig&quot;, &quot;breast&quot;, &quot;breast_quad&quot;, &quot;irradiat&quot;) # setting column names ) # presenting data in table form reactable( bcData, filterable = TRUE, compact = TRUE, bordered = TRUE, defaultPageSize = 5 ) "],["rpackage-twpackage.html", "Rpackage (twPackage)", " Rpackage (twPackage) Introduction The package consists of 4 functions for a DataScience portofolio. The package is created with the following guide: @handleywickhamPackages2ndEdition2023. Installation The package can be installed via github: - devtools::install_github(“ThijmenWeijgertze/twPackage”, build_vignettes = TRUE) The twPackage makes use of the following packages - here - reactable - readr - dplyr - magrittr Suggested packages: - knitr - rmarkdown Function and data explanation tw_factor() description: This command changes a column type to factor with unique levels using the unique() command usage: tw_factor(x) x: The column to be changed into a factor returns: Column with type factor tw_csv_rds() description: This command exports a variable containing a dataframe or tibble into a .csv and .rds file at a chosen location. The packages relies on the readr and here package. A project must be loaded in order to make this command work usage: tw_csv_rds(x, path) x Dataframe or tibble path The path where the files must be stored. The path must be specified starting at the rproject directory. Do not add .csv or .rds at the end of the file name returns: A .csv and .tsv from a dataframe or tibble at the chosen location tw_table() description: This command creates a basic table with the reactable package. usage: tw_table(x) x: A variable containing a dataframe or tibble returns: A basic table made with the reactable package tw_filter_select() description: This command creates a basic table with the reactable package. usage: tw_filter_select(x, filter_col, filter_value, select) x: The table to be filtered filter_col: The column where the filter_value can be found filter_value: The value to be filtered on select: The columns to be selected returns: A filtered and selected data Pokemon data description: A dataframe with 800 rows and 13 columns containing pokemon data from around 2016. usage: pokemon returns: The pokemon dataset source: Pokemon data Function examples loading package # load package library(twPackage) tw_factor() # Changing column Type.1 into a factor with unique levels pokemon$Type.1 &lt;- tw_factor(pokemon$Type.1) levels(pokemon$Type.1) ## [1] &quot;Grass&quot; &quot;Fire&quot; &quot;Water&quot; &quot;Bug&quot; &quot;Normal&quot; &quot;Poison&quot; ## [7] &quot;Electric&quot; &quot;Ground&quot; &quot;Fairy&quot; &quot;Fighting&quot; &quot;Psychic&quot; &quot;Rock&quot; ## [13] &quot;Ghost&quot; &quot;Ice&quot; &quot;Dragon&quot; &quot;Dark&quot; &quot;Steel&quot; &quot;Flying&quot; tw_csv_rds() # Exporting the pokemon table as pokemon.csv and pokemon.rds in the pokemon_data folder tw_csv_rds(pokemon, path = &quot;data_pokemon/pokemon&quot;) tw_table() # creating a table with 5 rows per page tw_table(pokemon, pagesize = 5) tw_filter_select() # Filtering on Type.1 = Fire and selecting the columns Name and Type.1 tw_filter_select( pokemon, filter_col = &quot;Type.1&quot;, filter_value = &quot;Fire&quot;, select = c(&quot;Name&quot;, &quot;Type.1&quot;) ) %&gt;% head(5) ## Name Type.1 ## 1 Charmander Fire ## 2 Charmeleon Fire ## 3 Charizard Fire ## 4 CharizardMega Charizard X Fire ## 5 CharizardMega Charizard Y Fire "],["antimicrobial-resistance-project.html", "Antimicrobial resistance project", " Antimicrobial resistance project Introduction During the Anti Microbial Resistance project, our project group worked on setting up a pipeline on behalf of the RIVM. The pipeline consists of a combination of the tools fastq-dump, fastqc, trimgalore, unicycler, plasmidEC, gplas2 and abricate. With these tools, the pipeline is able to take an SRA code with illumina data (short read) from a bacterial sample as input. After which it outputs the resistance genes in plasmids. During this project, version control of R packages, setting up a vignette and working with a github workflow were also taken into account. It has been decided to keep the github repository private. Principle Using a initiate.Rmd file all dependencies could be installed within a conda enviroment with the corresponding versions of all packages. SRA codes would be placed into a sra_input.txt file. Starting the pipeling Fastq-dump would cycle thorough the SRA codes and retrieve the SRA data from the database. Fastqc would make a report on quality control after the SRA data had been loaded. Besides the fastqc report another script would read the counts and lenghts of the short read fragments and make a data statistics report. Trim-galore would trim downstream if the q-score dropped too low. Unicycler would use the trimmed output and assemble the bacterial genome. PlasmidEC used the unicycler output to distinguish plasmid from genome. Gplas2 then used the data from unicycler and plasmidEC together to bin plasmid-predicted contigs based on sequence composition, coverage and assembly graph information. At last Abricate would use different resistance gene databases like NCBI to find resistence genes on the plasmid output from gplas2 and make a report on its findings. "],["analyzing-covid-19-data-using-parameters.html", "Analyzing Covid-19 data using parameters", " Analyzing Covid-19 data using parameters Selected parameters covidData parameter: data/CovidData.csv covidYear parameter: 2021 covidMonth parameter: 1 Inspecting data Pseudocode pyramid plot load the data select the right columns using the selected parameters change the geoId type to factor calculate the total cases and deaths per geoId divide the cases and deaths per geoId by the geoId’s population take the log1p of total cases and total deaths so that they can be plotted in the same ggplot make pyramid plot out of the total cases and total deaths https://r-statistics.co/Top50-Ggplot2-Visualizations-MasterList-R-Code.html # reading covidData parameter covidData &lt;- read.csv(params$covidData) # filtering on selected parameters covidDataFil &lt;- covidData %&gt;% dplyr::filter(year==params$covidYear &amp; month==params$covidMonth) # inspect the selected tables covidDataFil %&gt;% reactable::reactable(defaultPageSize = 5, compact = TRUE, filterable = TRUE) # make geoId a factor covidDataFil$geoId &lt;- factor(covidDataFil$geoId, levels = unique(covidDataFil$geoId)) # check type str(covidDataFil$geoId) ## Factor w/ 30 levels &quot;AT&quot;,&quot;BE&quot;,&quot;BG&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... # select the columns geoId and population cdGeoPop &lt;- covidDataFil %&gt;% select(geoId, countriesAndTerritories, popData2020) %&gt;% unique() # calulate total cases and deaths per geoId totalCases &lt;- aggregate(covidDataFil$cases, list(geoId = covidDataFil$geoId), sum) names(totalCases)[2] &lt;- &quot;totalCases&quot; totalDeaths &lt;- aggregate(covidDataFil$deaths, list(geoId = covidDataFil$geoId), sum) names(totalDeaths)[2] &lt;- &quot;totalDeaths&quot; # merge dataframes together like left join would in SQL covidDataJoined &lt;- cdGeoPop %&gt;% left_join(totalCases, by=&quot;geoId&quot;) %&gt;% left_join(totalDeaths, by=&quot;geoId&quot;) # normalize the data by dividing by population covidDataJoined &lt;- covidDataJoined %&gt;% mutate( casesNormalized=totalCases / popData2020, deathsNormalized=totalDeaths / popData2020) # death and cases in the same row in preparation for the plot (tidying data) covidDataJoined &lt;- covidDataJoined %&gt;% pivot_longer(cols=c(totalCases,totalDeaths), names_to = &quot;totalCases_totalDeaths&quot;, values_to = &quot;Normalized&quot;) # check the dataframe covidDataJoined ## # A tibble: 60 × 7 ## geoId countriesAndTerritories popData2020 casesNormalized deathsNormalized ## &lt;fct&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 AT Austria 8901064 0.00566 0.000208 ## 2 AT Austria 8901064 0.00566 0.000208 ## 3 BE Belgium 11522440 0.00521 0.000136 ## 4 BE Belgium 11522440 0.00521 0.000136 ## 5 BG Bulgaria 6951482 0.00237 0.000218 ## 6 BG Bulgaria 6951482 0.00237 0.000218 ## 7 HR Croatia 4058165 0.00532 0.000273 ## 8 HR Croatia 4058165 0.00532 0.000273 ## 9 CY Cyprus 888005 0.00926 0.0000912 ## 10 CY Cyprus 888005 0.00926 0.0000912 ## # ℹ 50 more rows ## # ℹ 2 more variables: totalCases_totalDeaths &lt;chr&gt;, Normalized &lt;int&gt; # make totalCases_totalDeaths a factor covidDataJoined$totalCases_totalDeaths &lt;- factor(covidDataJoined$totalCases_totalDeaths, levels = unique(covidDataJoined$totalCases_totalDeaths)) # check type str(covidDataJoined$totalCases_totalDeaths) ## Factor w/ 2 levels &quot;totalCases&quot;,&quot;totalDeaths&quot;: 1 2 1 2 1 2 1 2 1 2 ... # make pyramid plot # https://stackoverflow.com/questions/14680075/simpler-population-pyramid-in-ggplot2 library(ggplot2) covidDataJoined %&gt;% mutate(countriesAndTerritories_reordered = reorder(countriesAndTerritories, log1p(Normalized))) %&gt;% ggplot(aes( x = log1p(Normalized) * ifelse(totalCases_totalDeaths == &quot;totalDeaths&quot;, -1, 1), # if it&#39;s totalDeaths then multiply by -1 otherwise multiply by 1 y = countriesAndTerritories_reordered, fill = totalCases_totalDeaths)) + geom_col() + labs( title = paste( &quot;Impact of the corona pandemic as of&quot;, month(params$covidMonth, label = TRUE, abbr = FALSE), params$covidYear), x = &quot;log1p((Cases | Deaths) / population)&quot;, y = &quot;Countries &amp; Territories&quot;) + theme(axis.text.y = element_text(size = 8)) # text size y axis "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
