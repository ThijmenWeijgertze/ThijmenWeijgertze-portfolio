[["index.html", "Data Science portfolio Thijmen Weijgertze Main Page", " Data Science portfolio Thijmen Weijgertze Main Page Introduction This github page is written by Thijmen Weijgertze using Rstudio. I am a 3rd year Life Science student at HU University of Applied Sciences Utrecht. In the past half year I’ve followed a DataScience minor provided by the HU. This Github page serves as my DataScience portofolio for showing off my DataScience skills. The course is created by Gestel, Alyanne de Haan, and Marc Teunis (n.d.) based on the “bookdown: Authoring Books and Technical Documents with R Markdown” book from Xie (n.d.). Toc Main Page Meta Data DataScience CV Guerrilla Analytics C. Elegans plate experiment Open Peer Review Predicting breast cancer recurrence events using random forests Rpackage (twPackage) Antimicrobial resistance project Analyzing Covid-19 data using parameters Appendix 1: COVID-19 Vaccine paper Bibliography Contact information E-mail: thijmen.lifesciences@gmail.com GitHub: https://github.com/ThijmenWeijgertze LinkedIn: https://www.linkedin.com/in/thijmen-weijgertze-968a5a265/ "],["id_00.html", "MetaData", " MetaData Source code Bookdown source code: PortofolioBookdown Website source code: ThijmenWeijgertze-portofolio twPackage source code: twPackage Website link: thijmenweijgertze.github.io/ThijmenWeijgertze-portfolio/ How to build the book localy Download the bookdown package Download the bookdown source code Open the Rproject file Press the build book button If there is no build book button use: bookdown::render_book(\".\") If something goes wrong with downloading the dependencies automaticly try to install the dependencies manually Dependency packages devtools tidyverse tidymodels bookdown reactable gt fs tidyverse RColorBrewer ranger readxl ggfortify tableone randomForest broom.mixed rpart.plot vip MASS twPackage (devtools::install_github(“ThijmenWeijgertze/twPackage”, build_vignettes = TRUE)) Installing dependency packages (if it doesn’t download automatically please install the dependencies by hand): # if not installed: installing dependency packages from cran for (package in c(&quot;devtools&quot;, &quot;tidyverse&quot;, &quot;tidymodels&quot;, &quot;bookdown&quot;, &quot;reactable&quot;, &quot;gt&quot;, &quot;fs&quot;, &quot;tidyverse&quot;, &quot;RColorBrewer&quot;, &quot;ranger&quot;, &quot;readxl&quot;, &quot;ggfortify&quot;, &quot;tableone&quot;, &quot;randomForest&quot;, &quot;broom.mixed&quot;, &quot;rpart.plot&quot;, &quot;vip&quot;, &quot;MASS&quot;)) { if (!require(package, character.only = TRUE)) { install.packages(package, dependencies = TRUE) library(package, character.only = TRUE) } else { library(package, character.only = TRUE) } } # install twPackage from github if (!require(&quot;twPackage&quot;, character.only = TRUE)) { devtools::install_github(&quot;ThijmenWeijgertze/twPackage&quot;, build_vignettes = TRUE) library(package, character.only = TRUE) } Directory roadmap The structure of this portfolio’s source code is as follows: library(fs) library(tidyverse) dir_tree(&quot;.&quot;, recurse = TRUE, regexp = &quot;^.gitignore$|^[^_|(.git)]&quot;, all = TRUE) ## . ## ├── .gitignore ## ├── 00-METADATA.Rmd ## ├── 01_CV.Rmd ## ├── 02-GuerrillaAnalytics.Rmd ## ├── 03-CElegans.Rmd ## ├── 04-ReproducibleResearch.Rmd ## ├── 05-MachineLearning.Rmd ## ├── 06-Package.Rmd ## ├── 07-DataScienceProject.Rmd ## ├── 08-CovidCases.Rmd ## ├── 20-appendices.Rmd ## ├── 21-Bibliography.Rmd ## ├── css ## │ └── style.css ## ├── data ## │ ├── age.png ## │ ├── breast-cancer.data ## │ ├── breast-cancer.names ## │ ├── caseperm.png ## │ ├── CE-LIQ-FLOW-062_Tidydata.xlsx ## │ ├── CovidData.csv ## │ ├── Data_share_covid-19-vaccination_2020-10-04.csv ## │ ├── dice.csv ## │ ├── dice.rds ## │ ├── education.png ## │ ├── GuerrillaAnalytics.png ## │ ├── osfstorage-archive ## │ │ └── covid-vaccine-code_2020-10-04.Rmd ## │ ├── resampling.png ## │ └── supplementary.png ## ├── dataRaw ## │ ├── age.png ## │ ├── breast-cancer.data ## │ ├── breast-cancer.names ## │ ├── caseperm.png ## │ ├── CE.LIQ.FLOW.062_Tidydata.xlsx ## │ ├── data.csv ## │ ├── Data_share_covid-19-vaccination_2020-10-04.csv ## │ ├── dice.csv ## │ ├── dice.rds ## │ ├── education.png ## │ ├── GuerrillaAnalytics.png ## │ ├── osfstorage-archive.zip ## │ └── resampling.png ## ├── LICENSE ## ├── PortofolioBookdown.Rproj ## ├── README.md ## ├── References.bib ## └── scripts ## ├── CleanBook.R ## └── UpdateBib(DoesntWork).R Project directory folder .gitignore: file to withold files from being pushed to github 00-METADATA.Rmd: contains the metadata of the project 01_CV.Rmd: contains the CV 02-GuerillaAnalytics.Rmd: contains the folder structure of this project and an older project to show of guerilla analytics principles 03-CElegans.Rmd: contains the C.elegans plate analysis 04-ReproducibleResearch.Rmd: contains both the open peer review and the reproduced code 05-MachineLearning.Rmd: contains the breast cancer data analysis using random forest alghorithms from tidymodels 06-Package.Rmd: contains a selfmade r package called twPackage 07-DataScienceProject.Rmd: contains the information about the data science project AMR 08-CovidCases.Rmd: contains the covid cases analysis 20-appendices.Rmd: contains the appendix with the reproduced COVID-19 vaccin paper code 21-Bibliography.Rmd: contains the references css folder: contains the style.css file for the website (only works if it’s in the css folder) data folder: contains the data used in the code dataRaw folder: contains the original untouched data files index.Rmd: contains the index page (the page from where the bookdown starts) LICENSE: contains the MIT license PortofolioBookdown.Rproj: contains the Rproject README.md: redirects to the repositories and this METADATA file References.bib: contains the references used scripts folder: contains the scripts used in the code css folder style.css: consists of the css code for the bookdown layout dataRaw &amp; data folder The dataRaw folder holds the original data files. These data files are unedited and remain as untouched in the dataRaw folder as back-up. The data within the data folder are used within the Rmarkdown files and may differ from the original source in dataRaw. The data &amp; dataRawfolder folder consists of the following files: CE-LIQ-FLOW-062_Tidydata.xlsx: C.elegans data used in the C.elegans plate analysis Louter (n.d.) GuerrillaAnalytics.png: a screenshot of a project’s directory tree to demonstrate the guerrilla analytics principles education.png, age.png, caseperm.png, supplementary.png: screenshots used for reproducing r code (chapter open peer review) resampling.png: illustration of what resampling does in machine learning context covidData.csv: Covid data used in the Covid cases analysis “Data on the Daily Number of New Reported COVID-19 Cases and Deaths by EU/EEA Country” (2022) osfstorage-archive: contains the rmd of the reproduced code Palayew (2020) breast-cancer.data: contains the breast cancer data for the random forest algorithm chapter Matjaz Zwitter (1988) breast-cancer.names: contains the breast cancer data’s metadata Matjaz Zwitter (1988) dice.Rds and dice.csv: illustrate the tw_csv_rds function from the twPackage. Which saves a tibble as a csv and rds file scripts cleanBook.R: script to use the clean book function to clean up bookdown files after building the book. Usage: source(“data/CleanBook.R”) UpdateBib(DoesntWork).R: should have been used to update the References.bib file, but I could not get it working. I recommend ignoring this script "],["id_01.html", "DataScience CV", " DataScience CV About me I’m “Thijmen Weijgertze” (2003) from Ede, Gelderland. I tend to be very passionate about my study and love to talk about topics surrounding Life Sciences. As of writing this résumé I live with my parents, sister and our dog “Saartje” (breed: markiesje). In my free time my biggest hobby is music. I produce music from time to time and play piano almost daily. Besides music I also like to spent time with friends in the weekends and cycle so now and then. My education My most relevant skills related to DataScience (alphabetical order) Assembly tools I’ve worked with the assembly tools Unicycler, PlasmidEC and Gplas. These tools I’ve used within a project where we (our project group) assembled plasmids from bacterial illumina NGS data. This project was commissioned by the RIVM. Bash I’m capable of writing in Bash Cell biology I’ve theoretical knowledge about the principles of the cell such as cell metabolism and cellular communication Cell culture I’m capable of working and running experiments with cell cultures Git/Github I’ve worked with git/github workflows Immunology I’ve theoretical knowledge about the principles regarding immunology Metagenomics I’m capable of writing a pipeline for downstream metagenomics data NGS I’ve theoretical knowledge about the NGS principles of Pac-bio, Illumina and Nanopore Oncology I’ve theoretical knowledge about the hallmarks of tumorcells PCR I’m experienced with PCR both practicly and theoreticly Reproducibility I’m experienced regarding reproducible research R I’m capable of writing in R and experienced with rmarkdown. See my DataScience portofolio RNA/DNA isolation I’m capable of isolating RNA/DNA (including creating a cDNA bank) Random Forest Algorithms using tidymodels I’m capable of using Random Forests algorythms from tidymodels to solve classification problems RNA-seq I’m capable of writing a pipeline for downstream rna-seq analysis using DESeq2 (with results such as heatmaps, count plots, volcano plots, up/downregulated genes Statistics I’m capable of parametric and non-parametric tests and writing a conclusion based on the results Contact info www: https://thijmenweijgertze.github.io/ThijmenWeijgertze-portfolio/ email: thijmen.lifesciences@gmail.com github: https://github.com/ThijmenWeijgertze linkedin: https://www.linkedin.com/in/thijmen-weijgertze-968a5a265/ "],["id_02.html", "Guerrilla Analytics", " Guerrilla Analytics Example Project # Load example image knitr::include_graphics(here::here( &quot;data&quot;, &quot;GuerrillaAnalytics.png&quot; )) Root Of This Bookdown Project library(fs) library(tidyverse) dir_tree(&quot;.&quot;, recurse = TRUE, regexp = &quot;^.gitignore$|^[^_|(.git)]&quot;, all = TRUE) ## . ## ├── .gitignore ## ├── 00-METADATA.Rmd ## ├── 01_CV.Rmd ## ├── 02-GuerrillaAnalytics.Rmd ## ├── 03-CElegans.Rmd ## ├── 04-ReproducibleResearch.Rmd ## ├── 05-MachineLearning.Rmd ## ├── 06-Package.Rmd ## ├── 07-DataScienceProject.Rmd ## ├── 08-CovidCases.Rmd ## ├── 20-appendices.Rmd ## ├── 21-Bibliography.Rmd ## ├── css ## │ └── style.css ## ├── data ## │ ├── age.png ## │ ├── breast-cancer.data ## │ ├── breast-cancer.names ## │ ├── caseperm.png ## │ ├── CE-LIQ-FLOW-062_Tidydata.xlsx ## │ ├── CovidData.csv ## │ ├── Data_share_covid-19-vaccination_2020-10-04.csv ## │ ├── dice.csv ## │ ├── dice.rds ## │ ├── education.png ## │ ├── GuerrillaAnalytics.png ## │ ├── osfstorage-archive ## │ │ └── covid-vaccine-code_2020-10-04.Rmd ## │ ├── resampling.png ## │ └── supplementary.png ## ├── dataRaw ## │ ├── age.png ## │ ├── breast-cancer.data ## │ ├── breast-cancer.names ## │ ├── caseperm.png ## │ ├── CE.LIQ.FLOW.062_Tidydata.xlsx ## │ ├── data.csv ## │ ├── Data_share_covid-19-vaccination_2020-10-04.csv ## │ ├── dice.csv ## │ ├── dice.rds ## │ ├── education.png ## │ ├── GuerrillaAnalytics.png ## │ ├── osfstorage-archive.zip ## │ └── resampling.png ## ├── LICENSE ## ├── PortofolioBookdown.Rproj ## ├── README.md ## ├── References.bib ## └── scripts ## ├── CleanBook.R ## └── UpdateBib(DoesntWork).R "],["id_03.html", "C. Elegans plate experiment", " C. Elegans plate experiment Setup Setting a seed and loading packages # Seed chosen based on the current year set.seed(2024) # Loading packages library(tidyverse) library(RColorBrewer) library(readxl) library(here) library(reactable) Importing and inspecting the data According to The data was kindly supplied by J. Louter (INT/ILC) and was derived from an experiment in which adult C.elegans nematodes were exposed to varying concentrations of different compounds. Louter (n.d.) # importing xlsx file CE_LIQ_FLOW_062_Tidydata &lt;- read_excel( here::here( &quot;data&quot;, &quot;CE-LIQ-FLOW-062_Tidydata.xlsx&quot; ) ) # inspecting data in table format with the reactable package reactable(CE_LIQ_FLOW_062_Tidydata, defaultPageSize = 5, compact = TRUE) Scatterplot Pseudocode Deciding which columns will be included in the scatterplot Checking and possibly changing the datatypes of those columns Plotting the data in a scatterplot using ggplot Normalizing y-axis counts Setting the x-axis to a log10 scale Adding jitter to spread out points on top of eachother Checking and correcting datatypes needed for the scatterplot # Checking the data types of the following columns: RawData, compName, expType and compConcentration CE_LIQ_FLOW_062_Tidydata %&gt;% dplyr::select(RawData, compName, compConcentration, expType) %&gt;% str() ## tibble [360 × 4] (S3: tbl_df/tbl/data.frame) ## $ RawData : num [1:360] 44 37 45 47 41 35 41 36 40 38 ... ## $ compName : chr [1:360] &quot;2,6-diisopropylnaphthalene&quot; &quot;2,6-diisopropylnaphthalene&quot; &quot;2,6-diisopropylnaphthalene&quot; &quot;2,6-diisopropylnaphthalene&quot; ... ## $ compConcentration: chr [1:360] &quot;4.99&quot; &quot;4.99&quot; &quot;4.99&quot; &quot;4.99&quot; ... ## $ expType : chr [1:360] &quot;experiment&quot; &quot;experiment&quot; &quot;experiment&quot; &quot;experiment&quot; ... # Changing compConcentration to numeric; changing compName into a factor; changing expType into a factor CE_LIQ_FLOW_062_Tidydata$compConcentration &lt;- parse_number(CE_LIQ_FLOW_062_Tidydata$compConcentration) CE_LIQ_FLOW_062_Tidydata$compName &lt;- factor(CE_LIQ_FLOW_062_Tidydata$compName, levels = unique(CE_LIQ_FLOW_062_Tidydata$compName)) CE_LIQ_FLOW_062_Tidydata$expType &lt;- factor(CE_LIQ_FLOW_062_Tidydata$expType, levels = unique(CE_LIQ_FLOW_062_Tidydata$expType)) # Checking the new dataypes and factor levels CE_LIQ_FLOW_062_Tidydata %&gt;% dplyr::select(RawData, compName, compConcentration, expType) %&gt;% str() ## tibble [360 × 4] (S3: tbl_df/tbl/data.frame) ## $ RawData : num [1:360] 44 37 45 47 41 35 41 36 40 38 ... ## $ compName : Factor w/ 5 levels &quot;2,6-diisopropylnaphthalene&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... ## $ compConcentration: num [1:360] 4.99 4.99 4.99 4.99 4.99 4.99 4.99 4.99 4.99 4.99 ... ## $ expType : Factor w/ 4 levels &quot;experiment&quot;,&quot;controlPositive&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... levels(CE_LIQ_FLOW_062_Tidydata$compName) ## [1] &quot;2,6-diisopropylnaphthalene&quot; &quot;decane&quot; ## [3] &quot;naphthalene&quot; &quot;Ethanol&quot; ## [5] &quot;S-medium&quot; levels(CE_LIQ_FLOW_062_Tidydata$expType) ## [1] &quot;experiment&quot; &quot;controlPositive&quot; &quot;controlNegative&quot; &quot;controlVehicleA&quot; normalizing data CE_LIQ_FLOW_062_contNeg &lt;- CE_LIQ_FLOW_062_Tidydata %&gt;% filter(expType == &quot;controlNegative&quot;) contNeg_mean &lt;- mean(CE_LIQ_FLOW_062_contNeg$RawData) contNeg_mean ## [1] 85.9 CE_LIQ_FLOW_062_Tidydata &lt;- CE_LIQ_FLOW_062_Tidydata %&gt;% mutate(normalized = RawData/mean(contNeg_mean)) mean(CE_LIQ_FLOW_062_Tidydata$RawData) ## [1] NA CE_LIQ_FLOW_062_Tidydata %&gt;% dplyr::select(compName, expType, compConcentration, RawData, normalized) %&gt;% reactable(defaultPageSize = 5) CE_LIQ_FLOW_062_contNeg &lt;- CE_LIQ_FLOW_062_Tidydata %&gt;% filter(expType == &quot;controlNegative&quot;) contNegNorm_mean &lt;- mean(CE_LIQ_FLOW_062_contNeg$normalized) contNegNorm_mean ## [1] 1 # https://www.statology.org/how-to-normalize-data-in-r/ Plotting the CE_LIQ_FLOW_062_normalized in a scatterplot # Plotting the CE_LIQ_FLOW_062_normalized in a scatterplot ggplot( data = CE_LIQ_FLOW_062_Tidydata, aes(x = log10(compConcentration), y = normalized))+ geom_point( aes(color = compName, shape = expType), size = 1.5, alpha = 0.8, position = position_jitter(width = 0.1))+ # jitter to spread out values on top of eachother labs( title = &quot;Scatterplot CE_LIQ_FLOW_062_Tidydata&quot;, caption = &quot;Data normalized with the negative control mean&quot;, x = &quot;log10(compConcentration) in nM&quot;, y = &quot;RawData in counts&quot;)+ scale_color_brewer(palette = &quot;Dark2&quot;) # colorblind friendly color scale (#fig:scatterplot CElegans)The positive control for this experiments is controlPositive The negative control for this experiment is controlNegative Statistical tests Pseudocode Difference between the compound concentrations Loading the data Filter the concentrations per compound Plot the data using bargraph with the stdev as error bar Check the normality with a Shapio-Wilk test of both groups separately Perform an ANOVA test If the ANOVA test is significant, perform post hoc tests Draw a conclusion Difference between the LC50 curves Loading the data Filter the concentrations per compound Plotting the LC50 curves per compound Calculating 95% confidence intervals Checking the overlap between the 95% confidence intervals Draw conclusion "],["id_04.html", "Open Peer Review", " Open Peer Review Introduction The COVID-19 pandemic has highlighted how open science and reproducible research can speed up scientific progress. Open science makes it possible for different research facilities to share their finding with the rest of the world for others to build on. Reproducible research is an important subject within the Open Science. Not only should science publication be available to everyone (after the research has been completed) without any publisher pay wall, but it’s also important that publications are reproducible and the conclusions can be verified. In this chapter I’ll peer review an article from bioRxiv. The article chosen to review is: Exploring Metabolic Anomalies in COVID-19 and Post-COVID-19: A Machine Learning Approach with Explainable Artificial Intelligence Criteria Sumner et al. (2020) The open peer review will use criteria from the following publication: Reproducibility and reporting practices in COVID-19 preprint manuscripts Transparency Criteria Definition Response Type Study Purpose A concise statement in the introduction of the article, often in the last paragraph, that establishes the reason the research was conducted. Also called the study objective. Binary Data Availability Statement A statement, in an individual section offset from the main body of text, that explains how or if one can access a study’s data. The title of the section may vary, but it must explicitly mention data; it is therefore distinct from a supplementary materials section. Binary Data Location Where the article’s data can be accessed, either raw or processed. Found Value Study Location Author has stated in the methods section where the study took place or the data’s country/region of origin. Binary; Found Value Author Review The professionalism of the contact information that the author has provided in the manuscript. Found Value Ethics Statement A statement within the manuscript indicating any ethical concerns, including the presence of sensitive data. Binary Funding Statement A statement within the manuscript indicating whether or not the authors received funding for their research. Binary Code Availability Authors have shared access to the most updated code that they used in their study, including code used for analysis. Binary About the article The research publication Oropeza-Valdez et al. (2024) states in its abstract to have used techniques such as machine learning (ML), explainable artificial intelligence (XAI) and SHapley Additive exPlanations (SHAP) to analyze metabolic alterations in COVID-19 and Post-COVID-19 patients. The publication aims to uncover metabolic signatures and identify potential biomarkers for these conditions. It states to have analyzed 142 COVID-19, 48 Post-COVID-19 samples and 38 CONTROL patients, with 111 identified metabolites. After which the results have been compared to traditional methods such as PCA and PLS-DA. The comparison showed that the combined techniques of ML, XAI and SHAP outperformed the traditional methods and provides different insights in the metabolic basis of the disease’s progression and its aftermath. Open Peer Review Transparency Criteria Definition Response Type Study Purpose A concise statement in the introduction of the article, often in the last paragraph, that establishes the reason the research was conducted. Also called the study objective. yes Data Availability Statement A statement, in an individual section offset from the main body of text, that explains how or if one can access a study’s data. The title of the section may vary, but it must explicitly mention data; it is therefore distinct from a supplementary materials section. yes Data Location Where the article’s data can be accessed, either raw or processed. online Study Location Author has stated in the methods section where the study took place or the data’s country/region of origin. yes; Mendeley database Author Review The professionalism of the contact information that the author has provided in the manuscript. Tier 3 Ethics Statement A statement within the manuscript indicating any ethical concerns, including the presence of sensitive data. no Funding Statement A statement within the manuscript indicating whether or not the authors received funding for their research. yes Code Availability Authors have shared access to the most updated code that they used in their study, including code used for analysis. yes Conclusion The article scores positive in almost all aspects expects except the ethics statement and Author Review. The author scored tier 3 on the Author Review which states “Author used institutional email address as primary contact”. An ORCID ID has not been found which is required for Tier 4. The first time reviewing this article on April 17th 2024 the github page did not work yet and it scored negative on code availibility. This has recently been fixed as the github link now works. Thereby the article scores rather high on the open peer review. The article only misses an ethics statement and ORCID ID. Last updated on the 26th of May 2024. Reproducing R code Palayew (2020) Since the article above has written its code mainly in jupyter notebook using Google Collaboration. There has been decided to search another source for the reproducing R code section. The article chosen is COVID-19 Vaccine paper from the OSFHOME database. The code is reproduced in the Appendix 1: COVID-19 Vaccine paper “Open Peer Review” section to make this chapter better readable. # Load example image knitr::include_graphics(here::here( &quot;data&quot;, &quot;caseperm.png&quot; )) While reproducing the code a few issues were found. The code in the picture above did not work. This code was part of importing the health data. It did not work, because the column “caseperm” did not exist. However caseperm has also not been used in the entire code. Since the code runs perfectly fine without this column. Therefor it has been decided to put this part of the code as comments. The second issue arised after the code was executed inside this section. The select() function in other chapters after the execution of this code started breaking. To fix this the specifying select::select seemed to fix the issue. The code was later moved to the appendix and the issue didn’t matter anymore since it was on the end of the bookdown. At last I have made minimal adjustments to the headers in order for the layout to match the rest of the chapters and made ggsave a comment (to not save to plot as png). The paper starts off with loading the data and changing variable types to factors. The paper then asks two questions about vaccines to a group of people across different countries. The first question is as follows: “I would follow my employer’s recommendation to get a COVID-19 vaccine once the government has approved it as safe and effective”. 1179 people completely disagreed, 2299 people somewhat agreed, 3488 people were neutral or had no opinion, 4579 people somewhat agreed and 1881 people completely agreed. The data then got visualized using ggplot per category. This included gender, age, income, education and country. At last gender, age, income, education were visualized per country. See example screenshots below (or see the appendix). The country category however was the only one that had no title connected to its plot. # Load example image knitr::include_graphics(here::here( &quot;data&quot;, &quot;education.png&quot; )) # Load example image knitr::include_graphics(here::here( &quot;data&quot;, &quot;age.png&quot; )) The second question asked the next question: “If a COVID-19 vaccine is proven safe and effective and is available to me, I will take it”. 1091 people completely disagreed, 819 people somewhat agreed, 1912 people were neutral or had no opinion, 3318 people somewhat agreed and 6286 people completely agreed. The data got visualized in the same way as the first question. The country category again had no title. The paper then moves on to calculating regression using “Generalized Linear Models” (GLM) The GLM principle is based on the concept of linear models (calculating y with x) which can be computed using the lm() function. Lineair models however cannot handle things like binary types (example: yes/no) or counts, because they’re not continuous. GLM’s on the other hand are similar to linear models, but are able to compute other distributions then the normal distribution like poisson and binomial distributions. Furthermore they’re compatible with link functions. In this paper GLM is used because the data consists of count data and has a binomial distribution. Therefore is not compatible with a normal lineair model. Before the paper computes the GLM an if else statement is used to differentiate between positive and negative reaction to the questions. The glm() function within the paper is then used to compute the earlier explained GLM regression for a number of factors after which the coëfficients and confidence intervals per category are binded together. DataCamp (2020) At last a table with the percentage of all categories per country is printed out in a cat table format (like what percentage of people responded positive or what percentage was between the age of 18-24 in Brazil). The percentage of people which responded positive is also visualized using ggplot in a supplemental figure. See the screenshot below (or see the apendix). Again without a title. knitr::include_graphics(here::here( &quot;data&quot;, &quot;supplementary.png&quot; )) Conclusion on COVID-19 vaccin paper Reproducing the code was apart of some issues fairly easy to reproduce. However the paper did not have a clear structure. For example two different headers were both called “regression”. The paper heavily lacked comments and info around the code or within the rmd, which made it more difficult to understand. Within the paper some figures lacked titles and lastly the caseperm column did not exist and was never used. "],["id_05.html", "Predicting breast cancer recurrence events using random forests", " Predicting breast cancer recurrence events using random forests Introduction During the “Data Science for biology 2” course every student has the opportunity to learn a new skill for their portfolio. I have decided to introduce myself into “Machine Learning”. I haven chosen machine learning, because over the past decade it has become an important skill in DataScience. Implementing algorithms to predict medical outcomes, using available data sets, is important for early medical diagnoses. Furthermore machine learning makes it possible to analyze more factors and relations within the data at once. This can be highly beneficial while working with large data sets within the field of Life Sciences. Goal At the end of this course I want to have created an algorithm able to predict the recurrence of breast cancer using the breast-cancer data set. The algorithm will use the R package “tidymodels” for machine learning. Data This breast cancer domain was obtained from the University Medical Centre, Institute of Oncology, Ljubljana, Yugoslavia. Thanks go to M. Zwitter and M. Soklic for providing the data. Matjaz Zwitter (1988) The table consist of the following variables: Class: no-recurrence-events, recurrence-events age: 10-19, 20-29, 30-39, 40-49, 50-59, 60-69, 70-79, 80-89, 90-99. menopause: lt40, ge40, premeno. tumor-size: 0-4, 5-9, 10-14, 15-19, 20-24, 25-29, 30-34, 35-39, 40-44, 45-49, 50-54, 55-59. inv-nodes: 0-2, 3-5, 6-8, 9-11, 12-14, 15-17, 18-20, 21-23, 24-26, 27-29, 30-32, 33-35, 36-39. node-caps: yes, no. deg-malig: 1, 2, 3. breast: left, right. breast-quad: left-up, left-low, right-up, right-low, central. irradiat: yes, no. There are a total of 286 instances from which 201 no-recurrence-events and 85 recurrence-events. Planning I’ll spend 30 hours across 4 days on this skill across 2 weeks Week 1: - Day 1: Delving into the get started guide from tidymodels - Day 2: Delving into the get started guide from tidymodels Week 2: - Day 3: Apply the learned skills on the breast-cancer data set to predict the recurrence of breast cancer. - Day 4: Extension of past days Dataset library(gt) library(reactable) # loading in data bcData &lt;- read.csv( here::here( &quot;data&quot;, &quot;breast-cancer.data&quot; ), header = FALSE, # dataset does not contain column names col.names = c(&quot;class&quot;, &quot;age&quot;, &quot;menopause&quot;, &quot;tumor_size&quot;, &quot;inv_nodes&quot;, &quot;node_caps&quot;, &quot;deg_malig&quot;, &quot;breast&quot;, &quot;breast_quad&quot;, &quot;irradiat&quot;) # setting column names ) # presenting data in table form reactable( bcData, filterable = TRUE, compact = TRUE, bordered = TRUE, defaultPageSize = 5 ) Data Wrangling str(bcData) ## &#39;data.frame&#39;: 286 obs. of 10 variables: ## $ class : chr &quot;no-recurrence-events&quot; &quot;no-recurrence-events&quot; &quot;no-recurrence-events&quot; &quot;no-recurrence-events&quot; ... ## $ age : chr &quot;30-39&quot; &quot;40-49&quot; &quot;40-49&quot; &quot;60-69&quot; ... ## $ menopause : chr &quot;premeno&quot; &quot;premeno&quot; &quot;premeno&quot; &quot;ge40&quot; ... ## $ tumor_size : chr &quot;30-34&quot; &quot;20-24&quot; &quot;20-24&quot; &quot;15-19&quot; ... ## $ inv_nodes : chr &quot;0-2&quot; &quot;0-2&quot; &quot;0-2&quot; &quot;0-2&quot; ... ## $ node_caps : chr &quot;no&quot; &quot;no&quot; &quot;no&quot; &quot;no&quot; ... ## $ deg_malig : int 3 2 2 2 2 2 2 1 2 2 ... ## $ breast : chr &quot;left&quot; &quot;right&quot; &quot;left&quot; &quot;right&quot; ... ## $ breast_quad: chr &quot;left_low&quot; &quot;right_up&quot; &quot;left_low&quot; &quot;left_up&quot; ... ## $ irradiat : chr &quot;no&quot; &quot;no&quot; &quot;no&quot; &quot;no&quot; ... The breast cancer dataset had 10 columns and 286 rows. The next step was correcting the variable types to factors and renaming non-recurrence-events to NRE and recurrence events to RE. # changing type to factor bcData$class &lt;- factor(bcData$class, levels = unique(bcData$class)) bcData$age &lt;- factor(bcData$age, levels = unique(bcData$age)) bcData$menopause &lt;- factor(bcData$menopause, levels = unique(bcData$menopause)) bcData$tumor_size &lt;- factor(bcData$tumor_size, levels = unique(bcData$tumor_size)) bcData$inv_nodes &lt;- factor(bcData$inv_nodes, levels = unique(bcData$inv_nodes)) bcData$node_caps &lt;- factor(bcData$node_caps, levels = unique(bcData$node_caps)) bcData$deg_malig &lt;- factor(bcData$deg_malig, levels = unique(bcData$deg_malig)) bcData$breast &lt;- factor(bcData$breast, levels = unique(bcData$breast)) bcData$breast_quad &lt;- factor(bcData$breast_quad, levels = unique(bcData$breast_quad)) bcData$irradiat &lt;- factor(bcData$irradiat, levels = unique(bcData$irradiat)) # renaming values in class column bcData$class &lt;- factor(bcData$class, levels = c(&quot;no-recurrence-events&quot;, &quot;recurrence-events&quot;), labels = c(&quot;NRE&quot;, &quot;RE&quot;)) # using str to inspect the changed variables str(bcData) ## &#39;data.frame&#39;: 286 obs. of 10 variables: ## $ class : Factor w/ 2 levels &quot;NRE&quot;,&quot;RE&quot;: 1 1 1 1 1 1 1 1 1 1 ... ## $ age : Factor w/ 6 levels &quot;30-39&quot;,&quot;40-49&quot;,..: 1 2 2 3 2 3 4 3 2 2 ... ## $ menopause : Factor w/ 3 levels &quot;premeno&quot;,&quot;ge40&quot;,..: 1 1 1 2 1 2 1 2 1 1 ... ## $ tumor_size : Factor w/ 11 levels &quot;30-34&quot;,&quot;20-24&quot;,..: 1 2 2 3 4 3 5 2 6 2 ... ## $ inv_nodes : Factor w/ 7 levels &quot;0-2&quot;,&quot;6-8&quot;,&quot;9-11&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... ## $ node_caps : Factor w/ 3 levels &quot;no&quot;,&quot;yes&quot;,&quot;?&quot;: 1 1 1 1 1 1 1 1 1 1 ... ## $ deg_malig : Factor w/ 3 levels &quot;3&quot;,&quot;2&quot;,&quot;1&quot;: 1 2 2 2 2 2 2 3 2 2 ... ## $ breast : Factor w/ 2 levels &quot;left&quot;,&quot;right&quot;: 1 2 1 2 2 1 1 1 1 2 ... ## $ breast_quad: Factor w/ 6 levels &quot;left_low&quot;,&quot;right_up&quot;,..: 1 2 1 3 4 1 1 1 1 3 ... ## $ irradiat : Factor w/ 2 levels &quot;no&quot;,&quot;yes&quot;: 1 1 1 1 1 1 1 1 1 1 ... Splitting the data After checking the variables the data must be split into a training set and a test set. The test set must be different from the training set to avoid the model from overfitting. Overfitting means that the machine learning model is too well trained on memorizing its own training data that it can not accurately predict data outside of its training data. There are many ways to prevent overfitting. Examples are early stopping, feature selecting by excluding irralevent columns from the dataset, using more data to train on, data augmentation by adding noisy data within the dataset, Regularization by giving a penalty to parameters with large coefficients, ensamble methods by creating random samples in the training set to train them individually. This however comes with its own downside as this might cause underfitting. Underfitting is a term used when the model is not trained well enough on the training data to make accurate predictions outside of the training data. “What Is Overfitting? IBM” (2021); “Tidymodels - Welcome!” (n.d.) # split data library(tidymodels) set.seed(2024) bcData_split &lt;- initial_split(bcData, prop = 0.7, strata = class) The data gets split randomly so for the sake of reproducibility a seed is set. The function initial_split takes in the breast cancer dataset. Prop is used to specify the data to be split into 70% training data and 30% test data. Strata ensures that even though the data is split randomly both the training and test set will have the roughly the same percentage of no recurrence events and recurrence events. “Tidymodels - Welcome!” (n.d.) # store data into training and testing bcData_train &lt;- training(bcData_split) bcData_test &lt;- testing(bcData_split) Using the training and testing functions both the training and test set are stored in variables. # checking the split nrow(bcData_train)/nrow(bcData) ## [1] 0.6958042 # checking the equal proportions created by strata bcData_train %&gt;% count(class) %&gt;% mutate(prop = n/sum(n)) ## class n prop ## 1 NRE 140 0.7035176 ## 2 RE 59 0.2964824 # checking the equal proportions created by strata bcData_test %&gt;% count(class) %&gt;% mutate(prop = n/sum(n)) ## class n prop ## 1 NRE 61 0.7011494 ## 2 RE 26 0.2988506 Above results shows that as expected the dataset is split 70% to 30% and the strata function has made sure the proportions of no recurrence events and recurrence events are nearly equal in both the training and test set. Resampling the random forest model knitr::include_graphics(here::here( &quot;data&quot;, &quot;resampling.png&quot; )) Resampling will be used for evaluation of models. The data itself is split by using initial split. The training data is then split again into analysis and assesment groups. There are more ways to evaluate a model like cross-validation or bootstrapping. In this case cross-validation will be used. Cross-validation divides the training data in folds of around equal size. The first fold is the assessment fold and all other folds are analysis folds. While training each independent fold will be evaluated with the first assessment fold to predict its performance. “Tidymodels - Welcome!” (n.d.) set.seed(111) folds &lt;- vfold_cv(bcData_train, v = 15, strata = class) folds ## # 15-fold cross-validation using stratification ## # A tibble: 15 × 2 ## splits id ## &lt;list&gt; &lt;chr&gt; ## 1 &lt;split [185/14]&gt; Fold01 ## 2 &lt;split [185/14]&gt; Fold02 ## 3 &lt;split [185/14]&gt; Fold03 ## 4 &lt;split [185/14]&gt; Fold04 ## 5 &lt;split [185/14]&gt; Fold05 ## 6 &lt;split [186/13]&gt; Fold06 ## 7 &lt;split [186/13]&gt; Fold07 ## 8 &lt;split [186/13]&gt; Fold08 ## 9 &lt;split [186/13]&gt; Fold09 ## 10 &lt;split [186/13]&gt; Fold10 ## 11 &lt;split [186/13]&gt; Fold11 ## 12 &lt;split [186/13]&gt; Fold12 ## 13 &lt;split [186/13]&gt; Fold13 ## 14 &lt;split [186/13]&gt; Fold14 ## 15 &lt;split [187/12]&gt; Fold15 The vfold_cv function is used to create folds. Again a strata is used to distribute the no recurrence events and recurrence events equally over the folds. Creating a random forest model using Ranger show_engines(&quot;rand_forest&quot;) ## # A tibble: 6 × 2 ## engine mode ## &lt;chr&gt; &lt;chr&gt; ## 1 ranger classification ## 2 ranger regression ## 3 randomForest classification ## 4 randomForest regression ## 5 spark classification ## 6 spark regression There are a lot of engines to use for machine learning with tidymodels. A list of engines can be found here.Different engines use a different way of computing the data. Random forest algorithms try to narrow the data by asking questions about features such as is the age 15-19 or combine different features like age and degree of malignancy. These questions create a decision tree. Multiple decision trees combined makes up a forest. The questions get more specific the further in the decision tree you get. The questions itself are chosen based on the patterns in the dataset. The trained model then uses these same questions on new data sets such as the test set to predict in this case if its a no recurrence event or recurrence event. “What Is Random Forest? IBM” (2021); “Tidymodels - Welcome!” (n.d.) An ensemble technique called bagging (aka bootstrap aggregating) is often used to reduce variance in a dataset. The random forest algorithm uses this technique to improve its accuracy. The way bagging works is it makes multiple sample sets out of the training data. These sample sets can use the same values. Each sample is then trained independently and the results are combined to make a final more accurate prediction based on the averages of the trained samples. “What Is Random Forest? IBM” (2021); “Tidymodels - Welcome!” (n.d.) The random forest algorithm takes in three main hyperparameters: node size, number of trees and features sampled. If the number of values after a split in a tree drops below the node size then the tree will not split anymore. The number of trees specifies how many decision trees are combined to create the forest. The features sampled specifies which features (like age and degree of malignancy) will be taken into account for training the model. “What Is Random Forest? IBM” (2021); “Tidymodels - Welcome!” (n.d.); Simone (2015) # show available cores cores &lt;- parallel::detectCores() cores ## [1] 12 # train the model bcData_rf_ranger_mod &lt;- rand_forest( mode = &quot;classification&quot;, # the type of analysis: classification, regression, etc mtry = tune(), # amount of features per decision tree trees = tune(), # number of trees min_n = tune(), # minimal node size ) %&gt;% set_engine(&quot;ranger&quot;, num.threads = cores - 1, importance = &quot;impurity&quot;) # using the ranger engine # using all available cores except 1 # so that other smaller processes on the computer still run The random forest model is created using the rand_forest function. The mode is set to classification because the result is either a recurrence event or non recurrence event (another mode could be regression for example, which depends on the variable types within the dataset). The mtry (max features per decision tree) is set to tune() which means the random forest algorithm will try different mtry values to find the best performing value. Both the number of trees and the minimal node size are also set to tune(). The engine chosen is ranger. All cores except 1 are used for training the model. This computer has 12 cores so 11` will be used to train the model. By setting importance = “impurity” the most important decisions in the forest model are stored for visualization later on. “Tidymodels - Welcome!” (n.d.) Tuning and training the randomForest engine The random forest algorithm must know which features to use to predict the class. To tell the algorithm this a recipe must be made. bcData_rf_rec &lt;- recipe(class ~ ., data = bcData_train) %&gt;% step_zv(all_predictors()) The recipe contains a formula describing the to be predicted feature and the features to use for the prediction. In this case class is the feature to predict and all other features (indicated by the dot after the tilde) are used to predict the class. step_zv() is used to remove features in the training set that only show the same value (for example if all rows had age 15-19). These features are not useful and could only cause overfitting. With regression data “step_dummy()” must be used to convert categorical data to numerical data by making it like binary. In this case we don’t need it, because the breast cancer dataset is based on classification data. “Tidymodels - Welcome!” (n.d.) # combining model and recipe into a workflow bcData_ranger_rf_workflow &lt;- workflow() %&gt;% add_model(bcData_rf_ranger_mod) %&gt;% add_recipe(bcData_rf_rec) A workflow is created to combine the model with the recipe. After which the model will be tuned. set.seed(777) # tune the model bcData_ranger_lr_res &lt;- bcData_ranger_rf_workflow %&gt;% tune_grid(resamples = folds, grid = 50, control = control_grid(save_pred = TRUE), metrics = metric_set(roc_auc, accuracy)) bcData_ranger_lr_res ## # Tuning results ## # 15-fold cross-validation using stratification ## # A tibble: 15 × 5 ## splits id .metrics .notes .predictions ## &lt;list&gt; &lt;chr&gt; &lt;list&gt; &lt;list&gt; &lt;list&gt; ## 1 &lt;split [185/14]&gt; Fold01 &lt;tibble [100 × 7]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt; ## 2 &lt;split [185/14]&gt; Fold02 &lt;tibble [100 × 7]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt; ## 3 &lt;split [185/14]&gt; Fold03 &lt;tibble [100 × 7]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt; ## 4 &lt;split [185/14]&gt; Fold04 &lt;tibble [100 × 7]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt; ## 5 &lt;split [185/14]&gt; Fold05 &lt;tibble [100 × 7]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt; ## 6 &lt;split [186/13]&gt; Fold06 &lt;tibble [100 × 7]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt; ## 7 &lt;split [186/13]&gt; Fold07 &lt;tibble [100 × 7]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt; ## 8 &lt;split [186/13]&gt; Fold08 &lt;tibble [100 × 7]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt; ## 9 &lt;split [186/13]&gt; Fold09 &lt;tibble [100 × 7]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt; ## 10 &lt;split [186/13]&gt; Fold10 &lt;tibble [100 × 7]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt; ## 11 &lt;split [186/13]&gt; Fold11 &lt;tibble [100 × 7]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt; ## 12 &lt;split [186/13]&gt; Fold12 &lt;tibble [100 × 7]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt; ## 13 &lt;split [186/13]&gt; Fold13 &lt;tibble [100 × 7]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt; ## 14 &lt;split [186/13]&gt; Fold14 &lt;tibble [100 × 7]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt; ## 15 &lt;split [187/12]&gt; Fold15 &lt;tibble [100 × 7]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt; To tune the model a grid is needed. The grid could be made manually, but the function tune_grid directly makes the grid and tunes it. The resamples parameter is set to indicate that we’re using the resample folds from earlier. The grid parameter is set to 50 which means 50 models will be created with different combinations of hyperparameters. The control_grid parameter is set to save the predictions. The metrics parameter saves the roc_auc and accuracy results. “Tidymodels - Welcome!” (n.d.) autoplot(bcData_ranger_lr_res) Autoplot is used to show the effect of the hyperparameters on the accuracy and roc_auc results. # mtry + min_n bcData_ranger_lr_res %&gt;% collect_metrics() %&gt;% mutate(mtry = factor(mtry)) %&gt;% ggplot(aes(min_n, mean, color = mtry)) + geom_line(size = 1.5, alpha = 0.6) + geom_point(size = 2) + facet_wrap(~ .metric, scales = &quot;free&quot;, nrow = 2) + scale_x_log10(labels = scales::label_number()) + scale_color_viridis_d(option = &quot;plasma&quot;, begin = .9, end = 0) # mtry + trees bcData_ranger_lr_res %&gt;% collect_metrics() %&gt;% mutate(mtry = factor(mtry)) %&gt;% ggplot(aes(trees, mean, color = mtry)) + geom_line(size = 1.5, alpha = 0.6) + geom_point(size = 2) + facet_wrap(~ .metric, scales = &quot;free&quot;, nrow = 2) + scale_x_log10(labels = scales::label_number()) + scale_color_viridis_d(option = &quot;plasma&quot;, begin = .9, end = 0) # min_n + trees bcData_ranger_lr_res %&gt;% collect_metrics() %&gt;% mutate(min_n = factor(min_n)) %&gt;% ggplot(aes(trees, mean, color = min_n)) + geom_line(size = 1.5, alpha = 0.6) + geom_point(size = 2) + facet_wrap(~ .metric, scales = &quot;free&quot;, nrow = 2) + scale_x_log10(labels = scales::label_number()) + scale_color_viridis_d(option = &quot;plasma&quot;, begin = .9, end = 0) The figures above show the correlation between the hyperparameters and the metrics. The first figure shows the correlation between the mtry and min_n hyperparameters. The second figure shows the correlation between the mtry and trees hyperparameters. The third figure shows the correlation between the min_n and trees hyperparameters. The Y axis shows the mean of the metric and the X axis and colors show the hyperparameters. These figures can be used to see how certain parameters affect each other. For example mtry might have a really high mean, but a low mean when combined with a certain value of min_n. It also shows that the models tend to have the best performance with a mtry of 1 and a higher min_n. bcData_ranger_best_trees &lt;- bcData_ranger_lr_res %&gt;% show_best(metric = &quot;accuracy&quot;) bcData_ranger_best_trees ## # A tibble: 5 × 9 ## mtry trees min_n .metric .estimator mean n std_err .config ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 1 1877 10 accuracy binary 0.729 15 0.0154 Preprocessor1_Model… ## 2 1 833 10 accuracy binary 0.724 15 0.0171 Preprocessor1_Model… ## 3 8 298 35 accuracy binary 0.719 15 0.0254 Preprocessor1_Model… ## 4 8 94 18 accuracy binary 0.718 15 0.0273 Preprocessor1_Model… ## 5 4 1094 36 accuracy binary 0.714 15 0.0212 Preprocessor1_Model… Show_best() is used to show the top 5 best performing models based on the accuracy metric. “Tidymodels - Welcome!” (n.d.) bcData_ranger_tree &lt;- bcData_ranger_best_trees %&gt;% dplyr::select(&quot;mtry&quot;, &quot;trees&quot;, &quot;min_n&quot;, &quot;.config&quot;) %&gt;% filter(row_number() == 1) bcData_ranger_tree ## # A tibble: 1 × 4 ## mtry trees min_n .config ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; ## 1 1 1877 10 Preprocessor1_Model23 a model has been chosen using a combination of the select and filter Creating a random forest model using randomForest The same principle as ranger will be used, but now with the randomForest engine. # train the model bcData_rf_randomForest_mod &lt;- rand_forest( mtry = tune(), # amount of features per decision tree trees = tune(), # number of trees min_n = tune(), # minimal node size ) %&gt;% set_engine(&quot;randomForest&quot;, num.threads = cores-1) %&gt;% set_mode(&quot;classification&quot;) %&gt;% translate() # using the ranger engine # using all available cores except 1 # so that other smaller processes on the computer still run The engine uses 11 cores to train the model. The hyperparameters mtry, trees and min_n will be tuned later on using a grid. The translate function coverts the model to work with parsnip.@TidymodelsWelcome Tuning and training the randomForest engine # combining model and recipe into a workflow bcData_randomForest_rf_workflow &lt;- workflow() %&gt;% add_model(bcData_rf_randomForest_mod) %&gt;% add_recipe(bcData_rf_rec) The same recipe as last time is used, but this time the new randomForest model is added. set.seed(777) # tune the model bcData_randomForest_lr_res &lt;- bcData_randomForest_rf_workflow %&gt;% tune_grid(resamples = folds, grid = 50, control = control_grid(save_pred = TRUE), metrics = metric_set(roc_auc, accuracy)) bcData_randomForest_lr_res ## # Tuning results ## # 15-fold cross-validation using stratification ## # A tibble: 15 × 5 ## splits id .metrics .notes .predictions ## &lt;list&gt; &lt;chr&gt; &lt;list&gt; &lt;list&gt; &lt;list&gt; ## 1 &lt;split [185/14]&gt; Fold01 &lt;tibble [100 × 7]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt; ## 2 &lt;split [185/14]&gt; Fold02 &lt;tibble [100 × 7]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt; ## 3 &lt;split [185/14]&gt; Fold03 &lt;tibble [100 × 7]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt; ## 4 &lt;split [185/14]&gt; Fold04 &lt;tibble [100 × 7]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt; ## 5 &lt;split [185/14]&gt; Fold05 &lt;tibble [100 × 7]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt; ## 6 &lt;split [186/13]&gt; Fold06 &lt;tibble [100 × 7]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt; ## 7 &lt;split [186/13]&gt; Fold07 &lt;tibble [100 × 7]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt; ## 8 &lt;split [186/13]&gt; Fold08 &lt;tibble [100 × 7]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt; ## 9 &lt;split [186/13]&gt; Fold09 &lt;tibble [100 × 7]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt; ## 10 &lt;split [186/13]&gt; Fold10 &lt;tibble [100 × 7]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt; ## 11 &lt;split [186/13]&gt; Fold11 &lt;tibble [100 × 7]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt; ## 12 &lt;split [186/13]&gt; Fold12 &lt;tibble [100 × 7]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt; ## 13 &lt;split [186/13]&gt; Fold13 &lt;tibble [100 × 7]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt; ## 14 &lt;split [186/13]&gt; Fold14 &lt;tibble [100 × 7]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt; ## 15 &lt;split [187/12]&gt; Fold15 &lt;tibble [100 × 7]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt; The grid function is used to tune the model. The resamples parameter indicates that the earlier folds are being used. The grid parameter indicates the amount of models that will be made and compared for the best performance. Using the control and metrics parameters the predictions, roc_auc and accuracy are saved. “Tidymodels - Welcome!” (n.d.) autoplot(bcData_randomForest_lr_res) autoplot is used to show the effect of parameters on the accuracy and roc_auc results. # mtry + min_n bcData_randomForest_lr_res %&gt;% collect_metrics() %&gt;% mutate(mtry = factor(mtry)) %&gt;% ggplot(aes(min_n, mean, color = mtry)) + geom_line(size = 1.5, alpha = 0.6) + geom_point(size = 2) + facet_wrap(~ .metric, scales = &quot;free&quot;, nrow = 2) + scale_x_log10(labels = scales::label_number()) + scale_color_viridis_d(option = &quot;plasma&quot;, begin = .9, end = 0) # mtry + trees bcData_randomForest_lr_res %&gt;% collect_metrics() %&gt;% mutate(mtry = factor(mtry)) %&gt;% ggplot(aes(trees, mean, color = mtry)) + geom_line(size = 1.5, alpha = 0.6) + geom_point(size = 2) + facet_wrap(~ .metric, scales = &quot;free&quot;, nrow = 2) + scale_x_log10(labels = scales::label_number()) + scale_color_viridis_d(option = &quot;plasma&quot;, begin = .9, end = 0) # min_n + trees bcData_randomForest_lr_res %&gt;% collect_metrics() %&gt;% mutate(min_n = factor(min_n)) %&gt;% ggplot(aes(trees, mean, color = min_n)) + geom_line(size = 1.5, alpha = 0.6) + geom_point(size = 2) + facet_wrap(~ .metric, scales = &quot;free&quot;, nrow = 2) + scale_x_log10(labels = scales::label_number()) + scale_color_viridis_d(option = &quot;plasma&quot;, begin = .9, end = 0) The figures above show the correlation between the hyperparameters and the metrics. The first figure shows the correlation between the mtry and min_n hyperparameters. The second figure shows the correlation between the mtry and trees hyperparameters. The third figure shows the correlation between the min_n and trees hyperparameters. The Y axis shows the mean of the metric and the X axis and colors show the hyperparameters. These figures can be used to see how certain parameters affect eachother. For example mtry might have a really high mean, but a low mean when combined with a certain value of min_n. It also shows that like the ranger models the randomForest models again tend to have the best performance with a mtry of 1 and a higher min_n. bcData_randomForest_best_trees &lt;- bcData_randomForest_lr_res %&gt;% show_best(metric = &quot;accuracy&quot;) bcData_randomForest_best_trees ## # A tibble: 5 × 9 ## mtry trees min_n .metric .estimator mean n std_err .config ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 1 1877 10 accuracy binary 0.734 15 0.0145 Preprocessor1_Model… ## 2 1 833 10 accuracy binary 0.729 15 0.0157 Preprocessor1_Model… ## 3 1 54 34 accuracy binary 0.714 15 0.00713 Preprocessor1_Model… ## 4 2 1435 13 accuracy binary 0.709 15 0.0234 Preprocessor1_Model… ## 5 2 416 18 accuracy binary 0.708 15 0.0237 Preprocessor1_Model… Show_best() is used to show the top 5 best performing models based on the accuracy metric. bcData_randomForest_tree &lt;- bcData_randomForest_best_trees %&gt;% dplyr::select(&quot;mtry&quot;, &quot;trees&quot;, &quot;min_n&quot;, &quot;.config&quot;) %&gt;% filter(row_number() == 1) bcData_randomForest_tree ## # A tibble: 1 × 4 ## mtry trees min_n .config ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; ## 1 1 1877 10 Preprocessor1_Model23 A model has been chosen using a combination of the select and filter Final models comparison The best performing models are compared to each other using the finalize_workflow() function. This function overwrites the earlier workflow with the parameters from the best performing model. The last_fit() function is then used to fit the model to the test set and see the performance. “Tidymodels - Welcome!” (n.d.) # using the best performing ranger model bcData_ranger_final_workflow &lt;- bcData_ranger_rf_workflow %&gt;% finalize_workflow(bcData_ranger_tree) # fit ranger model bcData_ranger_final_fit &lt;- bcData_ranger_final_workflow %&gt;% last_fit(bcData_split) # using the best performing randomForest model bcData_randomForest_final_workflow &lt;- bcData_randomForest_rf_workflow %&gt;% finalize_workflow(bcData_randomForest_tree) # fit randomForest model bcData_randomForest_final_fit &lt;- bcData_randomForest_final_workflow %&gt;% last_fit(bcData_split) # ranger metrics bcData_ranger_final_fit %&gt;% collect_metrics() ## # A tibble: 3 × 4 ## .metric .estimator .estimate .config ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 accuracy binary 0.747 Preprocessor1_Model1 ## 2 roc_auc binary 0.799 Preprocessor1_Model1 ## 3 brier_class binary 0.168 Preprocessor1_Model1 # randomForest metrics bcData_randomForest_final_fit %&gt;% collect_metrics() ## # A tibble: 3 × 4 ## .metric .estimator .estimate .config ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 accuracy binary 0.770 Preprocessor1_Model1 ## 2 roc_auc binary 0.775 Preprocessor1_Model1 ## 3 brier_class binary 0.171 Preprocessor1_Model1 The accuracy shows how many values of the test set were correctly predicted The ROC AUC is the area under the ROC curve. The ROC curve itself is made by plotting the true positive rate against the false positive rate. This shows the ability of the model to distinquish between non-recurrence-events and recurrence-events. A model can have a high accuracy even though the roc_auc is low. An example could be that a large proportion of the data is non-recurrence-events and the model then keeps guessing non-recurrence events, because it has a higher chance to be correct. The brier_class is used to determine the overall performance of the model. The lower the brier_class the better the model. The values range between 0 and 1. This shows that the ranger model has the best overall performance. “Tidymodels - Welcome!” (n.d.) # predictions ranger bcData_ROC_ranger &lt;- bcData_ranger_final_fit %&gt;% collect_predictions() %&gt;% roc_curve(class, .pred_NRE) %&gt;% mutate(model = &quot;ranger&quot;) # predictions randomForest bcData_ROC_randomForest &lt;- bcData_randomForest_final_fit %&gt;% collect_predictions() %&gt;% roc_curve(class, .pred_NRE) %&gt;% mutate(model = &quot;randomForest&quot;) # bind the data together in one plot bind_rows(bcData_ROC_ranger, bcData_ROC_randomForest) %&gt;% ggplot(aes(x = 1 - specificity, y = sensitivity, col = model)) + geom_path(lwd = 1.5, alpha = 0.8) + geom_abline(lty = 3) + coord_equal() + scale_color_viridis_d(option = &quot;plasma&quot;, end = .6) The figure above this paragraph is the ROC. Both randomForest and ranger have very similar results though ranger performs slightly better. bcData_ranger_final_tree &lt;- extract_workflow(bcData_ranger_final_fit) bcData_ranger_final_tree ## ══ Workflow [trained] ═══════════════════════════════════════════════════ ## Preprocessor: Recipe ## Model: rand_forest() ## ## ── Preprocessor ───────────────────────────────────────────────────────── ## 1 Recipe Step ## ## • step_zv() ## ## ── Model ──────────────────────────────────────────────────────────────── ## Ranger result ## ## Call: ## ranger::ranger(x = maybe_data_frame(x), y = y, mtry = min_cols(~1L, x), num.trees = ~1877L, min.node.size = min_rows(~10L, x), num.threads = ~cores - 1, importance = ~&quot;impurity&quot;, verbose = FALSE, seed = sample.int(10^5, 1), probability = TRUE) ## ## Type: Probability estimation ## Number of trees: 1877 ## Sample size: 199 ## Number of independent variables: 9 ## Mtry: 1 ## Target node size: 10 ## Variable importance mode: impurity ## Splitrule: gini ## OOB prediction error (Brier s.): 0.1942307 bcData_randomForest_final_tree &lt;- extract_workflow(bcData_randomForest_final_fit) bcData_randomForest_final_tree ## ══ Workflow [trained] ═══════════════════════════════════════════════════ ## Preprocessor: Recipe ## Model: rand_forest() ## ## ── Preprocessor ───────────────────────────────────────────────────────── ## 1 Recipe Step ## ## • step_zv() ## ## ── Model ──────────────────────────────────────────────────────────────── ## ## Call: ## randomForest(x = maybe_data_frame(x), y = y, ntree = ~1877L, mtry = min_cols(~1L, x), nodesize = min_rows(~10L, x), num.threads = ~cores - 1) ## Type of random forest: classification ## Number of trees: 1877 ## No. of variables tried at each split: 1 ## ## OOB estimate of error rate: 28.14% ## Confusion matrix: ## NRE RE class.error ## NRE 137 3 0.02142857 ## RE 53 6 0.89830508 https://www.tidymodels.org/learn/models/conformal-regression/ library(vip) bcData_ranger_final_tree %&gt;% extract_fit_parsnip() %&gt;% vip() bcData_randomForest_final_tree %&gt;% extract_fit_parsnip() %&gt;% vip() The vip function is used to show the most important features in the model. The ranger model shows that the most important features for it’s model are the degree of malignancy, then the tumor size, etc. The randomForest model shows that the most important feature is the tumor_size and then the inv_nodes, etc. Both graphs show that it’s not of importance which breast the tumor is located in. "],["id_06.html", "Rpackage (twPackage)", " Rpackage (twPackage) Introduction The package consists of 4 functions for a DataScience portofolio. The package is created with the following guide: Handley Wickham and Jennifer Bryan (2023). Installation The package can be installed via github: - devtools::install_github(“ThijmenWeijgertze/twPackage”, build_vignettes = TRUE) The twPackage makes use of the following packages - here - reactable - readr - dplyr - magrittr Suggested packages: - knitr - rmarkdown Function and data explanation tw_factor() description: This command changes a column type to factor with unique levels using the unique() command usage: tw_factor(x) x: The column to be changed into a factor returns: Column with type factor tw_csv_rds() description: This command exports a variable containing a dataframe or tibble into a .csv and .rds file at a chosen location. The packages relies on the readr and here package. A project must be loaded in order to make this command work usage: tw_csv_rds(x, path) x Dataframe or tibble path The path where the files must be stored. The path must be specified starting at the rproject directory. Do not add .csv or .rds at the end of the file name returns: A .csv and .tsv from a dataframe or tibble at the chosen location tw_table() description: This command creates a basic table with the reactable package. usage: tw_table(x) x: A variable containing a dataframe or tibble returns: A basic table made with the reactable package tw_filter_select() description: This command creates a basic table with the reactable package. usage: tw_filter_select(x, filter_col, filter_value, select) x: The table to be filtered filter_col: The column where the filter_value can be found filter_value: The value to be filtered on select: The columns to be selected returns: A filtered and selected data Pokemon data description: A dataframe with 800 rows and 13 columns containing pokemon data from around 2016. usage: pokemon returns: The pokemon dataset source: Pokemon data Function examples loading package # load package library(twPackage) tw_factor() # Changing column Type.1 into a factor with unique levels pokemon$Type.1 &lt;- tw_factor(pokemon$Type.1) levels(pokemon$Type.1) ## [1] &quot;Grass&quot; &quot;Fire&quot; &quot;Water&quot; &quot;Bug&quot; &quot;Normal&quot; &quot;Poison&quot; ## [7] &quot;Electric&quot; &quot;Ground&quot; &quot;Fairy&quot; &quot;Fighting&quot; &quot;Psychic&quot; &quot;Rock&quot; ## [13] &quot;Ghost&quot; &quot;Ice&quot; &quot;Dragon&quot; &quot;Dark&quot; &quot;Steel&quot; &quot;Flying&quot; tw_csv_rds() # Creating a table with dice data dice_table &lt;- tibble( Number = 1:10, Die_1 = c(3, 2, 3, 1, 5, 3, 2, 2, 6, 4), Die_2 = c(3, 2, 5, 4, 2, 6, 1, 5, 3, 6) ) # Exporting the dice table as dice.csv and dice.rds in the data folder tw_csv_rds(dice_table, path = &quot;data/dice&quot;) tw_table() # creating a table with 5 rows per page tw_table(pokemon, pagesize = 5) tw_filter_select() # Filtering on Type.1 = Fire and selecting the columns Name and Type.1 tw_filter_select( pokemon, filter_col = &quot;Type.1&quot;, filter_value = &quot;Fire&quot;, select = c(&quot;Name&quot;, &quot;Type.1&quot;) ) %&gt;% head(5) ## Name Type.1 ## 1 Charmander Fire ## 2 Charmeleon Fire ## 3 Charizard Fire ## 4 CharizardMega Charizard X Fire ## 5 CharizardMega Charizard Y Fire "],["id_07.html", "Antimicrobial resistance project", " Antimicrobial resistance project Introduction During the Anti Microbial Resistance project, our project group worked on setting up a pipeline on behalf of the RIVM. The pipeline consists of a combination of the tools fastq-dump, fastqc, trimgalore, unicycler, plasmidEC, gplas2 and abricate. With these tools, the pipeline is able to take an SRA code with illumina data (short read) from a bacterial sample as input. After which it outputs the resistance genes in plasmids. During this project, version control of R packages, setting up a vignette and working with a github workflow were also taken into account. It has been decided to keep the github repository private. Principle Using a initiate.Rmd file all dependencies could be installed within a conda enviroment with the corresponding versions of all packages. SRA codes would be placed into a sra_input.txt file. Starting the pipeling Fastq-dump would cycle thorough the SRA codes and retrieve the SRA data from the database. Fastqc would make a report on quality control after the SRA data had been loaded. Besides the fastqc report another script would read the counts and lenghts of the short read fragments and make a data statistics report. Trim-galore would trim downstream if the q-score dropped too low. Unicycler would use the trimmed output and assemble the bacterial genome. PlasmidEC used the unicycler output to distinguish plasmid from genome. Gplas2 then used the data from unicycler and plasmidEC together to bin plasmid-predicted contigs based on sequence composition, coverage and assembly graph information. At last Abricate would use different resistance gene databases like NCBI to find resistence genes on the plasmid output from gplas2 and make a report on its findings Paganini et al. (2024); “Babraham Bioinformatics - Trim Galore!” (n.d.); Wick et al. (2017); Seemann (2024). "],["id_08.html", "Analyzing Covid-19 data using parameters", " Analyzing Covid-19 data using parameters Data source: “Data on the Daily Number of New Reported COVID-19 Cases and Deaths by EU/EEA Country” (2022) Selected parameters covidData parameter: data/CovidData.csv covidYear parameter: 2020 covidMonth parameter: 4 Psuedocode pyramid plot Pseudocode pyramid plot load the data select the right columns using the selected parameters change the geoId type to factor calculate the total cases and deaths per geoId divide the cases and deaths per geoId by the geoId’s population take the log1p of total cases and total deaths so that they can be plotted in the same ggplot make pyramid plot out of the total cases and total deaths alex (2022); Eeeeed (2019); Elferts (2013); guyabel (2016); “Top 50 Ggplot2 Visualizations - The Master List (With Full R Code)” (n.d.) Data reading # reading covidData parameter covidData &lt;- read.csv(params$covidData) # filtering on selected parameters covidDataFil &lt;- covidData %&gt;% dplyr::filter(year==params$covidYear &amp; month==params$covidMonth) # inspect the selected tables covidDataFil %&gt;% reactable::reactable(defaultPageSize = 5, compact = TRUE, filterable = TRUE) prepare the data for the pyramid plot # make geoId a factor covidDataFil$geoId &lt;- factor(covidDataFil$geoId, levels = unique(covidDataFil$geoId)) # check type str(covidDataFil$geoId) ## Factor w/ 30 levels &quot;AT&quot;,&quot;BE&quot;,&quot;BG&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... # select the columns geoId and population cdGeoPop &lt;- covidDataFil %&gt;% dplyr::select(geoId, countriesAndTerritories, popData2020) %&gt;% unique() # calulate total cases and deaths per geoId totalCases &lt;- aggregate(covidDataFil$cases, list(geoId = covidDataFil$geoId), sum) names(totalCases)[2] &lt;- &quot;totalCases&quot; totalDeaths &lt;- aggregate(covidDataFil$deaths, list(geoId = covidDataFil$geoId), sum) names(totalDeaths)[2] &lt;- &quot;totalDeaths&quot; # merge dataframes together like left join would in SQL covidDataJoined &lt;- cdGeoPop %&gt;% left_join(totalCases, by=&quot;geoId&quot;) %&gt;% left_join(totalDeaths, by=&quot;geoId&quot;) # normalize the data by dividing by population covidDataJoined &lt;- covidDataJoined %&gt;% mutate( casesNormalized=totalCases / popData2020, deathsNormalized=totalDeaths / popData2020) # death and cases in the same row in preparation for the plot (tidying data) covidDataJoined &lt;- covidDataJoined %&gt;% pivot_longer(cols=c(totalCases,totalDeaths), names_to = &quot;totalCases_totalDeaths&quot;, values_to = &quot;Normalized&quot;) # check the dataframe covidDataJoined ## # A tibble: 60 × 7 ## geoId countriesAndTerritories popData2020 casesNormalized deathsNormalized ## &lt;fct&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 AT Austria 8901064 0.000562 0.0000504 ## 2 AT Austria 8901064 0.000562 0.0000504 ## 3 BE Belgium 11522440 0.00291 0.000563 ## 4 BE Belgium 11522440 0.00291 0.000563 ## 5 BG Bulgaria 6951482 0.000162 0.00000834 ## 6 BG Bulgaria 6951482 0.000162 0.00000834 ## 7 HR Croatia 4058165 0.000298 0.0000155 ## 8 HR Croatia 4058165 0.000298 0.0000155 ## 9 CY Cyprus 888005 0.000662 0.0000101 ## 10 CY Cyprus 888005 0.000662 0.0000101 ## # ℹ 50 more rows ## # ℹ 2 more variables: totalCases_totalDeaths &lt;chr&gt;, Normalized &lt;int&gt; # make totalCases_totalDeaths a factor covidDataJoined$totalCases_totalDeaths &lt;- factor(covidDataJoined$totalCases_totalDeaths, levels = unique(covidDataJoined$totalCases_totalDeaths)) # check type str(covidDataJoined$totalCases_totalDeaths) ## Factor w/ 2 levels &quot;totalCases&quot;,&quot;totalDeaths&quot;: 1 2 1 2 1 2 1 2 1 2 ... Pyramid plot # make pyramid plot # https://stackoverflow.com/questions/14680075/simpler-population-pyramid-in-ggplot2 library(ggplot2) covidDataJoined %&gt;% mutate(countriesAndTerritories_reordered = reorder(countriesAndTerritories, Normalized)) %&gt;% ggplot(aes( # if it&#39;s totalDeaths then multiply by -1 otherwise multiply by 1. Log1p is used to avoid log(0) x = log1p(Normalized) * ifelse(totalCases_totalDeaths == &quot;totalDeaths&quot;, -1, 1), y = countriesAndTerritories_reordered, fill = totalCases_totalDeaths)) + geom_col() + labs( title = paste( &quot;Impact of the corona pandemic as of&quot;, month(params$covidMonth, label = TRUE, abbr = FALSE), params$covidYear), x = &quot;log1p((Cases | Deaths) / population2020)&quot;, y = &quot;Countries &amp; Territories&quot;) + theme(axis.text.y = element_text(size = 8)) + # text size y axis scale_color_brewer(palette = &quot;Dark2&quot;) # colorblind friendly color scale Pseudocode heatmap plot Raschka (09:00:00 +0000) devide the cases and deaths by the population data of 2020 to normalize the data sort the columns by day number Bobbitt (2021) use pivor wider to give every day a column in preparation for the heatmap store the names of the countries and territories Tomas (2014) convert all day columns to numeric convert type to matrix and assign it the stored names plot the heatmap Preparing the data for the heatmap Cases Raschka (09:00:00 +0000): # matrix preparation for cases covidDataMatrixCases &lt;- covidDataFil %&gt;% mutate(Normalized = cases/popData2020) %&gt;% dplyr::select(countriesAndTerritories, day, Normalized) # sort dataframe so that the days are ordered from 1 to 30 or 31 covidDataMatrixCases &lt;- covidDataMatrixCases[order(covidDataMatrixCases$day), ] # column per day covidDataMatrixCases &lt;- covidDataMatrixCases %&gt;% pivot_wider(names_from = day, values_from = Normalized) # store the names because the matrix only wants one type (numerical, string, etc..) # drop=TRUE simplifies the values to store it only as a vector and not as a column of a dataframe rnames &lt;- covidDataMatrixCases[,1, drop=TRUE] # convert all columns except the first to numeric covidDataMatrixCases &lt;- data.matrix(covidDataMatrixCases[,2:ncol(covidDataMatrixCases)]) # assign column names to matrix rownames(covidDataMatrixCases) &lt;- rnames Deaths Raschka (09:00:00 +0000): # matrix preparation for cases covidDataMatrixDeaths &lt;- covidDataFil %&gt;% mutate(Normalized = deaths/popData2020) %&gt;% dplyr::select(countriesAndTerritories, day, Normalized) # https://www.statology.org/sort-dataframe-by-column-in-r/ covidDataMatrixDeaths &lt;- covidDataMatrixDeaths[order(covidDataMatrixDeaths$day), ] # column per day covidDataMatrixDeaths &lt;- covidDataMatrixDeaths %&gt;% pivot_wider(names_from = day, values_from = Normalized) # store the names because the matrix only wants one type (numerical, string, etc..) # drop=TRUE simplifies the values to store it only as a vector and not as a column of a dataframe rnames &lt;- covidDataMatrixDeaths[,1, drop=TRUE] # convert all columns except the first to numeric covidDataMatrixDeaths &lt;- data.matrix(covidDataMatrixDeaths[,2:ncol(covidDataMatrixDeaths)]) # assign column names to matrix rownames(covidDataMatrixDeaths) &lt;- rnames Heatmap Raschka (09:00:00 +0000) # heatmap plot cases heatmap( covidDataMatrixCases, # data Rowv=NA, Colv=NA, # no clustering main = paste(&quot;Cases/population2020 per day as of&quot;, month(params$covidMonth, label = TRUE, abbr = FALSE), params$covidYear), # title name xlab = &quot;Day&quot;, # x-axis name ylab = &quot;Country or Territory&quot; # y-axis name ) # heatmap plot deaths heatmap( covidDataMatrixDeaths, # data Rowv=NA, Colv=NA, # no clustering main = paste(&quot;Deaths/population2020 per day as of&quot;, month(params$covidMonth, label = TRUE, abbr = FALSE), params$covidYear), # title name xlab = &quot;Day&quot;, # x-axis name ylab = &quot;Country or Territory&quot; # y-axis name ) "],["id_20.html", "Appendix 1: COVID-19 Vaccine paper", " Appendix 1: COVID-19 Vaccine paper see: Open Peer Review loading and wrangling data library(tidyverse) library(MASS) data &lt;- read.csv(here::here(&quot;data/Data_share_covid-19-vaccination_2020-10-04.csv&quot;)) #fix all data$Gender_r &lt;- factor(ifelse( data$Gender == 1, &quot;Male&quot;, ifelse(data$Gender == 2, &quot;Female&quot;, &quot;Other&quot;) ),ordered = FALSE) data$within_fct &lt;- factor(data$within_country,levels = c( 1,2,3,4,5,6), labels = c( &quot;&lt;20%&quot;, &quot;20-40%&quot;, &quot;40-60%&quot;, &quot;60-80%&quot;, &quot;&gt;80%&quot;, &quot;Refused&quot; )) data$ww_fct &lt;- factor(data$world_wide, levels = c( 1,2,3,4,6), c( &quot;&lt;$2 per day&quot;, &quot;$2-$8 per day&quot;, &quot;$8-$32 per day&quot;, &quot;$32+&quot;, &quot;Refused&quot; )) data$educ_fact &lt;- factor(data$Universal_edu, levels = c( 1,2,3,4 ), labels = c( &quot;Less than high school&quot;, &quot;High school some college&quot;, &quot;Bachelor&quot;, &quot;Post Graduate&quot; )) data$country_name &lt;- as.factor(ifelse( #this is by far one of the most egregios pieces of code I have written please forgive me I was tired and hit a wall and now I am too lazy to change it to something more elegant data$Country == 1,&quot;Brazil&quot;, ifelse(data$Country == 2, &quot;Canada&quot;,ifelse( data$Country == 3, &quot;China&quot;,ifelse( data$Country == 4, &quot;Ecuador&quot;,ifelse( data$Country == 5, &quot;France&quot;,ifelse( data$Country == 6, &quot;Germany&quot;,ifelse( data$Country == 7, &quot;India&quot;,ifelse( data$Country == 8, &quot;Italy&quot;,ifelse( data$Country == 9, &quot;Mexico&quot;,ifelse( data$Country == 10, &quot;Nigeria&quot;,ifelse( data$Country == 11, &quot;Poland&quot;,ifelse( data$Country == 12, &quot;Russia&quot;,ifelse( data$Country == 13, &quot;South Africa&quot;, ifelse( data$Country == 14, &quot;South Korea&quot;,ifelse( data$Country == 15, &quot;Singapore&quot;, ifelse( data$Country == 16, &quot;Spain&quot;, ifelse( data$Country == 17, &quot;Sweden&quot;,ifelse( data$Country == 18, &quot;United Kingdom&quot;,ifelse( data$Country == 19, &quot;United States&quot;,&quot;l&quot; )))))))))))))))))))) data$agegroup_fct &lt;- factor(x = data$Age_grou, levels = c(1, 2, 3, 4), labels = c(&quot;18-24&quot;, &quot;25-54&quot;, &quot;55-64&quot;, &quot;65+&quot;), ordered = FALSE) #mortlity factor data$mortality_fct &lt;- cut(as.numeric(data$mortalityperm), c(-Inf,200,400,Inf),labels = c( &quot;low&quot;, &quot;medium&quot;, &quot;high&quot; )) #!!!casesperm does not exist in the data!!!### #case factor #data$case_fct &lt;- cut(as.numeric(data$casesperm), # c(-Inf,2000,4000,Inf),labels = c( # &quot;low&quot;, # &quot;medium&quot;, # &quot;high&quot; # )) ###just the index vars data_test &lt;- data[,4:13] ###make group data # need to make pop covid and need to make pop COVID Question:I would follow my employer’s recommendation to get a COVID-19 vaccine once the government has approved it as safe and effective. data$Busines2_fct &lt;- factor(data$Business2, levels = c( 1, 2, 3, 4, 5 ), labels = c( &quot;Completely disagree&quot;, &quot;Somewhat disagree&quot;, &quot;Neutral/no opinion&quot;, &quot;Somewhat agree&quot;, &quot;Completely agree&quot; )) summary(data$Busines2_fct) ## Completely disagree Somewhat disagree Neutral/no opinion Somewhat agree ## 1179 2299 3488 4579 ## Completely agree ## 1881 #busi by gender gender &lt;- ggplot( data = data )+ geom_bar(aes( x = Gender_r, fill = Busines2_fct ), stat = &#39;count&#39;, position = &#39;fill&#39;)+ theme(axis.text.x = element_text(angle = 90, size = 10))+ coord_flip()+ labs(title = &quot;Business by gender&quot;)+ xlab(&quot;Proportion&quot;)+ ylab(&quot;Gender&quot;)+ scale_fill_brewer(&quot;Response&quot;, type = &quot;div&quot;, palette = 4) gender #busi by age age &lt;- ggplot( data = data )+ geom_bar(aes( x = agegroup_fct, fill = Busines2_fct ), stat = &#39;count&#39;, position = &#39;fill&#39;)+ theme(axis.text.x = element_text(angle = 90, size = 10))+ coord_flip()+ labs(title = &quot;Business by age&quot;)+ ylab(&quot;Proportion&quot;)+ xlab(&quot;Age&quot;)+ scale_fill_brewer(&quot;Response&quot;, type = &quot;div&quot;, palette = 4) age ##busi by income inc &lt;- ggplot( data = data )+ geom_bar(aes( x = ww_fct, fill = Busines2_fct ), stat = &#39;count&#39;, position = &#39;fill&#39;)+ theme(axis.text.x = element_text(angle = 90, size = 10))+ coord_flip()+ labs(title = &quot;Business by income&quot;)+ ylab(&quot;Proportion&quot;)+ xlab(&quot;Income level&quot;)+ scale_fill_brewer(&quot;Response&quot;, type = &quot;div&quot;, palette = 4) inc ##busi by education educ &lt;- ggplot( data = data )+ geom_bar(aes( x = educ_fact, fill = Busines2_fct ), stat = &#39;count&#39;, position = &#39;fill&#39;)+ theme(axis.text.x = element_text(angle = 90, size = 10))+ coord_flip()+ labs(title = &quot;Business by education&quot;)+ ylab(&quot;Proportion&quot;)+ xlab(&quot;Education level&quot;)+ scale_fill_brewer(&quot;Response&quot;, type = &quot;div&quot;, palette = 4) educ #simple country plot country &lt;- data %&gt;% count(country_name = country_name, Busines2_fct = Busines2_fct)%&gt;% mutate(pct = prop.table(n)) %&gt;% ggplot(aes(x = country_name, y = pct,label = scales::percent(pct, accuracy = 3)))+ geom_bar(aes( fill = Busines2_fct ), position = &#39;fill&#39;, stat = &quot;identity&quot;)+ #geom_text(position = &#39;fill&#39;) + theme(axis.text.x = element_text(angle = 90, size = 10))+ coord_flip()+ #labs(title = &quot;Business by country&quot;)+ ylab(&quot;Proportion&quot;)+ xlab(&quot;Country&quot;)+ scale_fill_brewer(&quot;Response&quot;, type = &quot;div&quot;, palette = 4) country #ggsave(filename = &quot;figure-bus-vax.pdf&quot;, plot = country, path = &quot;covid-score/vaccine paper/&quot;, width = 10, height = 7) #busi by coutry x gender gender &lt;- ggplot( data = data )+ geom_bar(aes( x = country_name, fill = Busines2_fct ), stat = &#39;count&#39;, position = &#39;fill&#39;)+ facet_wrap(~Gender_r)+ theme(axis.text.x = element_text(angle = 90, size = 10))+ coord_flip()+ labs(title = &quot;Business by country by gender&quot;)+ ylab(&quot;Proportion&quot;)+ xlab(&quot;Country&quot;)+ scale_fill_brewer(&quot;Response&quot;, type = &quot;div&quot;, palette = 4) gender #country by age age &lt;- ggplot( data = data )+ geom_bar(aes( x = country_name, fill = Busines2_fct ), stat = &#39;count&#39;, position = &#39;fill&#39;)+ facet_wrap(~agegroup_fct)+ theme(axis.text.x = element_text(angle = 90, size = 10))+ coord_flip()+ labs(title = &quot;Business by country by age&quot;)+ ylab(&quot;Proportion&quot;)+ xlab(&quot;Country&quot;)+ scale_fill_brewer(&quot;Response&quot;, type = &quot;div&quot;, palette = 4) age #country by income inc &lt;- ggplot( data = data )+ geom_bar(aes( x = country_name, fill = Busines2_fct ), stat = &#39;count&#39;, position = &#39;fill&#39;)+ facet_wrap(~ww_fct)+ theme(axis.text.x = element_text(angle = 90, size = 10))+ coord_flip()+ labs(title = &quot;Business by country by income&quot;)+ ylab(&quot;Proportion&quot;)+ xlab(&quot;Country&quot;)+ scale_fill_brewer(&quot;Response&quot;, type = &quot;div&quot;, palette = 4) inc #country by education educ &lt;- ggplot( data = data )+ geom_bar(aes( x = country_name, fill = Busines2_fct ), stat = &#39;count&#39;, position = &#39;fill&#39;)+ facet_wrap(~educ_fact)+ theme(axis.text.x = element_text(angle = 90, size = 10))+ coord_flip()+ labs(title = &quot;Business by country by education&quot;)+ ylab(&quot;Proportion&quot;)+ xlab(&quot;Country&quot;)+ scale_fill_brewer(&quot;Response&quot;, type = &quot;div&quot;, palette = 4) educ Question:If a COVID-19 vaccine is proven safe and effective and is available to me, I will take it. data$Vaccine_fct &lt;- factor(data$Vaccine, levels = c( 1, 2, 3, 4, 5 ), labels = c( &quot;Completely disagree&quot;, &quot;Somewhat disagree&quot;, &quot;Neutral/no opinion&quot;, &quot;Somewhat agree&quot;, &quot;Completely agree&quot; )) summary(data$Vaccine_fct) ## Completely disagree Somewhat disagree Neutral/no opinion Somewhat agree ## 1091 819 1912 3318 ## Completely agree ## 6286 #Vaccine_fct by gender gender &lt;- ggplot( data = data )+ geom_bar(aes( x = Gender_r, fill = Vaccine_fct ), stat = &#39;count&#39;, position = &#39;fill&#39;)+ theme(axis.text.x = element_text(angle = 90, size = 10))+ coord_flip()+ labs(title = &quot;Vaccine by gender&quot;)+ ylab(&quot;Proportion&quot;)+ xlab(&quot;Gender&quot;)+ scale_fill_brewer(&quot;Response&quot;, type = &quot;div&quot;, palette = 4) gender #Vaccine_fct by age age &lt;- ggplot( data = data )+ geom_bar(aes( x = agegroup_fct, fill = Vaccine_fct ), stat = &#39;count&#39;, position = &#39;fill&#39;)+ theme(axis.text.x = element_text(angle = 90, size = 10))+ coord_flip()+ labs(title = &quot;Vaccine by age group&quot;)+ ylab(&quot;Proportion&quot;)+ xlab(&quot;Age&quot;)+ scale_fill_brewer(&quot;Response&quot;, type = &quot;div&quot;, palette = 4) age ##Vaccine_fct by income inc &lt;- ggplot( data = data )+ geom_bar(aes( x = ww_fct, fill = Vaccine_fct ), stat = &#39;count&#39;, position = &#39;fill&#39;)+ theme(axis.text.x = element_text(angle = 90, size = 10))+ coord_flip()+ labs(title = &quot;Vaccine by income&quot;)+ ylab(&quot;Proportion&quot;)+ xlab(&quot;Income&quot;)+ scale_fill_brewer(&quot;Response&quot;, type = &quot;div&quot;, palette = 4) inc ##Vaccine_fct by education educ &lt;- ggplot( data = data )+ geom_bar(aes( x = educ_fact, fill = Vaccine_fct ), stat = &#39;count&#39;, position = &#39;fill&#39;)+ theme(axis.text.x = element_text(angle = 90, size = 10))+ coord_flip()+ labs(title = &quot;Vaccine by education&quot;)+ ylab(&quot;Proportion&quot;)+ xlab(&quot;Education&quot;)+ scale_fill_brewer(&quot;Response&quot;, type = &quot;div&quot;, palette = 4) educ ##vaccine by country country &lt;- data %&gt;% count(country_name = country_name, Vaccine_fct = Vaccine_fct)%&gt;% mutate(pct = prop.table(n)) %&gt;% ggplot(aes(x = country_name, y = pct,label = scales::percent(pct, accuracy = 3)))+ geom_bar(aes( fill = Vaccine_fct ), position = &#39;fill&#39;, stat = &quot;identity&quot;)+ #geom_text(position = &#39;fill&#39;) + theme(axis.text.x = element_text(angle = 90, size = 10))+ coord_flip()+ #labs(title = &quot;Business by country&quot;)+ ylab(&quot;Proportion&quot;)+ xlab(&quot;Country&quot;)+ scale_fill_brewer(&quot;Response&quot;, type = &quot;div&quot;, palette = 4) country #ggsave(filename = &quot;figure-vax-vax.pdf&quot;, plot = country, path = &quot;covid-score/vaccine paper/&quot;, width = 10, height = 7) gender &lt;- ggplot( data = data )+ geom_bar(aes( x = country_name, fill = Vaccine_fct ), stat = &#39;count&#39;, position = &#39;fill&#39;)+ facet_wrap(~Gender_r)+ theme(axis.text.x = element_text(angle = 90, size = 10))+ coord_flip()+ labs(title = &quot;Vaccine by country by gender&quot;)+ ylab(&quot;Proportion&quot;)+ xlab(&quot;Country&quot;)+ scale_fill_brewer(&quot;Response&quot;, type = &quot;div&quot;, palette = 4) gender age &lt;- ggplot( data = data )+ geom_bar(aes( x = country_name, fill = Vaccine_fct ), stat = &#39;count&#39;, position = &#39;fill&#39;)+ facet_wrap(~agegroup_fct)+ theme(axis.text.x = element_text(angle = 90, size = 10))+ coord_flip()+ labs(title = &quot;Vaccine by country by age&quot;)+ ylab(&quot;Proportion&quot;)+ xlab(&quot;Country&quot;)+ scale_fill_brewer(&quot;Response&quot;, type = &quot;div&quot;, palette = 4) age inc &lt;- ggplot( data = data )+ geom_bar(aes( x = country_name, fill = Vaccine_fct ), stat = &#39;count&#39;, position = &#39;fill&#39;)+ facet_wrap(~ww_fct)+ theme(axis.text.x = element_text(angle = 90, size = 10))+ coord_flip()+ labs(title = &quot;Vaccine by country by income&quot;)+ ylab(&quot;Proportion&quot;)+ xlab(&quot;Country&quot;)+ scale_fill_brewer(&quot;Response&quot;, type = &quot;div&quot;, palette = 4) inc educ &lt;- ggplot( data = data )+ geom_bar(aes( x = country_name, fill = Vaccine_fct ), stat = &#39;count&#39;, position = &#39;fill&#39;)+ facet_wrap(~educ_fact)+ theme(axis.text.x = element_text(angle = 90, size = 10))+ coord_flip()+ labs(title = &quot;Vaccine by country by education&quot;)+ ylab(&quot;Proportion&quot;)+ xlab(&quot;Country&quot;)+ scale_fill_brewer(&quot;Response&quot;, type = &quot;div&quot;, palette = 4) educ regression data$biz_reg &lt;- ifelse(data$Business2 &gt; 3, 1, 0) fit2 &lt;- glm(data = data, formula = biz_reg ~ educ_fact, family=binomial(link=&quot;logit&quot;)) summary(fit2) ## ## Call: ## glm(formula = biz_reg ~ educ_fact, family = binomial(link = &quot;logit&quot;), ## data = data) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -0.24137 0.03255 -7.415 1.22e-13 *** ## educ_factHigh school some college 0.22944 0.04373 5.247 1.55e-07 *** ## educ_factBachelor 0.21647 0.04629 4.676 2.92e-06 *** ## educ_factPost Graduate 0.27021 0.06673 4.049 5.14e-05 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 18549 on 13394 degrees of freedom ## Residual deviance: 18513 on 13391 degrees of freedom ## (31 observations deleted due to missingness) ## AIC: 18521 ## ## Number of Fisher Scoring iterations: 3 exp(cbind(coef(fit2), confint(fit2))) ## 2.5 % 97.5 % ## (Intercept) 0.7855478 0.7369295 0.8372369 ## educ_factHigh school some college 1.2578937 1.1546207 1.3705237 ## educ_factBachelor 1.2416827 1.1340265 1.3596568 ## educ_factPost Graduate 1.3102448 1.1496353 1.4934216 fit3 &lt;- glm(data = data, formula = biz_reg ~ ww_fct, family=binomial(link=&quot;logit&quot;)) summary(fit3) ## ## Call: ## glm(formula = biz_reg ~ ww_fct, family = binomial(link = &quot;logit&quot;), ## data = data) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -0.31121 0.09574 -3.250 0.00115 ** ## ww_fct$2-$8 per day -0.09922 0.11888 -0.835 0.40395 ## ww_fct$8-$32 per day 0.03515 0.10257 0.343 0.73187 ## ww_fct$32+ 0.38609 0.09818 3.933 8.4e-05 *** ## ww_fctRefused -0.24903 0.12660 -1.967 0.04918 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 18593 on 13425 degrees of freedom ## Residual deviance: 18451 on 13421 degrees of freedom ## AIC: 18461 ## ## Number of Fisher Scoring iterations: 4 exp(cbind(coef(fit3), confint(fit3))) ## 2.5 % 97.5 % ## (Intercept) 0.7325581 0.6065818 0.8830920 ## ww_fct$2-$8 per day 0.9055477 0.7175302 1.1436626 ## ww_fct$8-$32 per day 1.0357699 0.8476987 1.2675333 ## ww_fct$32+ 1.4712151 1.2145868 1.7851499 ## ww_fctRefused 0.7795590 0.6082024 0.9992052 fit4 &lt;- glm(data = data, formula = biz_reg ~ Gender_r, family=binomial(link=&quot;logit&quot;)) summary(fit4) ## ## Call: ## glm(formula = biz_reg ~ Gender_r, family = binomial(link = &quot;logit&quot;), ## data = data) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -0.04351 0.02362 -1.842 0.0655 . ## Gender_rMale -0.06788 0.03482 -1.949 0.0513 . ## Gender_rOther -0.38862 0.21242 -1.830 0.0673 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 18549 on 13394 degrees of freedom ## Residual deviance: 18543 on 13392 degrees of freedom ## (31 observations deleted due to missingness) ## AIC: 18549 ## ## Number of Fisher Scoring iterations: 3 exp(cbind(coef(fit4), confint(fit4))) ## 2.5 % 97.5 % ## (Intercept) 0.9574236 0.9140984 1.002787 ## Gender_rMale 0.9343727 0.8727181 1.000365 ## Gender_rOther 0.6779892 0.4437462 1.023462 fit5 &lt;- glm(data = data, formula = biz_reg ~ agegroup_fct, family=binomial(link=&quot;logit&quot;)) summary(fit5) ## ## Call: ## glm(formula = biz_reg ~ agegroup_fct, family = binomial(link = &quot;logit&quot;), ## data = data) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -0.22949 0.04439 -5.170 2.34e-07 *** ## agegroup_fct25-54 0.19504 0.04949 3.941 8.11e-05 *** ## agegroup_fct55-64 0.15310 0.06822 2.244 0.0248 * ## agegroup_fct65+ 0.12029 0.06835 1.760 0.0784 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 18549 on 13394 degrees of freedom ## Residual deviance: 18533 on 13391 degrees of freedom ## (31 observations deleted due to missingness) ## AIC: 18541 ## ## Number of Fisher Scoring iterations: 3 exp(cbind(coef(fit5), confint(fit5))) ## 2.5 % 97.5 % ## (Intercept) 0.7949389 0.7285878 0.8670821 ## agegroup_fct25-54 1.2153557 1.1031284 1.3393090 ## agegroup_fct55-64 1.1654375 1.0195955 1.3322025 ## agegroup_fct65+ 1.1278247 0.9864098 1.2895293 data$sick_reg &lt;- ifelse(data$sickwithCOVID == 1, 1, 0) fit6 &lt;- glm(data = data, formula = biz_reg ~ sick_reg, family=binomial(link=&quot;logit&quot;)) summary(fit6) ## ## Call: ## glm(formula = biz_reg ~ sick_reg, family = binomial(link = &quot;logit&quot;), ## data = data) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -0.07712 0.01861 -4.145 3.4e-05 *** ## sick_reg 0.01237 0.05006 0.247 0.805 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 18593 on 13425 degrees of freedom ## Residual deviance: 18593 on 13424 degrees of freedom ## AIC: 18597 ## ## Number of Fisher Scoring iterations: 3 exp(cbind(coef(fit6), confint(fit6))) ## 2.5 % 97.5 % ## (Intercept) 0.925778 0.8926175 0.960154 ## sick_reg 1.012450 0.9177857 1.116795 fit7 &lt;- glm(data = data, formula = biz_reg ~ covidcases, family=binomial(link=&quot;logit&quot;)) summary(fit7) ## ## Call: ## glm(formula = biz_reg ~ covidcases, family = binomial(link = &quot;logit&quot;), ## data = data) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -6.447e-02 2.024e-02 -3.186 0.00144 ** ## covidcases -3.536e-08 3.412e-08 -1.036 0.29998 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 18593 on 13425 degrees of freedom ## Residual deviance: 18592 on 13424 degrees of freedom ## AIC: 18596 ## ## Number of Fisher Scoring iterations: 3 exp(cbind(coef(fit7), confint(fit7))) ## 2.5 % 97.5 % ## (Intercept) 0.9375602 0.9010938 0.9754905 ## covidcases 1.0000000 0.9999999 1.0000000 fit8 &lt;- glm(data = data, formula = biz_reg ~ mortalityperm, family=binomial(link=&quot;logit&quot;)) summary(fit8) ## ## Call: ## glm(formula = biz_reg ~ mortalityperm, family = binomial(link = &quot;logit&quot;), ## data = data) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 9.383e-02 2.489e-02 3.771 0.000163 *** ## mortalityperm -7.526e-04 7.986e-05 -9.424 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 18593 on 13425 degrees of freedom ## Residual deviance: 18504 on 13424 degrees of freedom ## AIC: 18508 ## ## Number of Fisher Scoring iterations: 3 exp(cbind(coef(fit8), confint(fit8))) ## 2.5 % 97.5 % ## (Intercept) 1.0983768 1.0461005 1.153293 ## mortalityperm 0.9992477 0.9990912 0.999404 fit9 &lt;- glm(data = data, formula = biz_reg ~ trusthealth, family=binomial(link=&quot;logit&quot;)) summary(fit9) ## ## Call: ## glm(formula = biz_reg ~ trusthealth, family = binomial(link = &quot;logit&quot;), ## data = data) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -1.11093 0.03560 -31.21 &lt;2e-16 *** ## trusthealth 1.46964 0.04143 35.47 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 18593 on 13425 degrees of freedom ## Residual deviance: 17192 on 13424 degrees of freedom ## AIC: 17196 ## ## Number of Fisher Scoring iterations: 4 exp(cbind(coef(fit9), confint(fit9))) ## 2.5 % 97.5 % ## (Intercept) 0.329253 0.306936 0.352903 ## trusthealth 4.347664 4.009819 4.716956 regression data$vacc2_reg &lt;- ifelse(data$Vaccine &gt; 3, 1, 0) fit2 &lt;- glm(data = data, formula = vacc2_reg ~ educ_fact, family=binomial(link=&quot;logit&quot;)) summary(fit2) ## ## Call: ## glm(formula = vacc2_reg ~ educ_fact, family = binomial(link = &quot;logit&quot;), ## data = data) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 0.72940 0.03449 21.148 &lt; 2e-16 *** ## educ_factHigh school some college 0.23498 0.04750 4.947 7.53e-07 *** ## educ_factBachelor 0.28923 0.05078 5.696 1.23e-08 *** ## educ_factPost Graduate 0.37261 0.07564 4.926 8.38e-07 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 16005 on 13394 degrees of freedom ## Residual deviance: 15959 on 13391 degrees of freedom ## (31 observations deleted due to missingness) ## AIC: 15967 ## ## Number of Fisher Scoring iterations: 4 exp(cbind(coef(fit2), confint(fit2))) ## 2.5 % 97.5 % ## (Intercept) 2.073836 1.938783 2.219479 ## educ_factHigh school some college 1.264886 1.152443 1.388312 ## educ_factBachelor 1.335394 1.209004 1.475298 ## educ_factPost Graduate 1.451515 1.252847 1.685387 fit3 &lt;- glm(data = data, formula = vacc2_reg ~ ww_fct, family=binomial(link=&quot;logit&quot;)) summary(fit3) ## ## Call: ## glm(formula = vacc2_reg ~ ww_fct, family = binomial(link = &quot;logit&quot;), ## data = data) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 0.28377 0.09555 2.970 0.00298 ** ## ww_fct$2-$8 per day 0.31961 0.11974 2.669 0.00761 ** ## ww_fct$8-$32 per day 0.62649 0.10370 6.042 1.53e-09 *** ## ww_fct$32+ 0.77846 0.09872 7.885 3.14e-15 *** ## ww_fctRefused -0.09912 0.12463 -0.795 0.42645 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 16039 on 13425 degrees of freedom ## Residual deviance: 15864 on 13421 degrees of freedom ## AIC: 15874 ## ## Number of Fisher Scoring iterations: 4 exp(cbind(coef(fit3), confint(fit3))) ## 2.5 % 97.5 % ## (Intercept) 1.3281250 1.102059 1.603210 ## ww_fct$2-$8 per day 1.3765894 1.088341 1.740585 ## ww_fct$8-$32 per day 1.8710240 1.525768 2.291495 ## ww_fct$32+ 2.1781143 1.793335 2.641425 ## ww_fctRefused 0.9056355 0.709063 1.155941 fit4 &lt;- glm(data = data, formula = vacc2_reg ~ Gender_r, family=binomial(link=&quot;logit&quot;)) summary(fit4) ## ## Call: ## glm(formula = vacc2_reg ~ Gender_r, family = binomial(link = &quot;logit&quot;), ## data = data) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 1.01412 0.02672 37.957 &lt; 2e-16 *** ## Gender_rMale -0.17404 0.03858 -4.511 6.45e-06 *** ## Gender_rOther -1.49105 0.21385 -6.972 3.12e-12 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 16005 on 13394 degrees of freedom ## Residual deviance: 15940 on 13392 degrees of freedom ## (31 observations deleted due to missingness) ## AIC: 15946 ## ## Number of Fisher Scoring iterations: 4 exp(cbind(coef(fit4), confint(fit4))) ## 2.5 % 97.5 % ## (Intercept) 2.7569408 2.6168378 2.9057887 ## Gender_rMale 0.8402641 0.7790644 0.9062713 ## Gender_rOther 0.2251371 0.1468455 0.3405870 fit5 &lt;- glm(data = data, formula = vacc2_reg ~ agegroup_fct, family=binomial(link=&quot;logit&quot;)) summary(fit5) ## ## Call: ## glm(formula = vacc2_reg ~ agegroup_fct, family = binomial(link = &quot;logit&quot;), ## data = data) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 0.77449 0.04745 16.324 &lt; 2e-16 *** ## agegroup_fct25-54 0.11185 0.05320 2.103 0.0355 * ## agegroup_fct55-64 0.18343 0.07479 2.453 0.0142 * ## agegroup_fct65+ 0.54982 0.07943 6.922 4.44e-12 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 16005 on 13394 degrees of freedom ## Residual deviance: 15950 on 13391 degrees of freedom ## (31 observations deleted due to missingness) ## AIC: 15958 ## ## Number of Fisher Scoring iterations: 4 exp(cbind(coef(fit5), confint(fit5))) ## 2.5 % 97.5 % ## (Intercept) 2.169492 1.977863 2.382223 ## agegroup_fct25-54 1.118340 1.007203 1.240767 ## agegroup_fct55-64 1.201332 1.037878 1.391520 ## agegroup_fct65+ 1.732948 1.484222 2.026518 fit6 &lt;- glm(data = data, formula = vacc2_reg ~ sick_reg, family=binomial(link=&quot;logit&quot;)) summary(fit6) ## ## Call: ## glm(formula = vacc2_reg ~ sick_reg, family = binomial(link = &quot;logit&quot;), ## data = data) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 0.92490 0.02062 44.864 &lt;2e-16 *** ## sick_reg -0.02516 0.05522 -0.456 0.649 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 16039 on 13425 degrees of freedom ## Residual deviance: 16039 on 13424 degrees of freedom ## AIC: 16043 ## ## Number of Fisher Scoring iterations: 4 exp(cbind(coef(fit6), confint(fit6))) ## 2.5 % 97.5 % ## (Intercept) 2.5216068 2.4220314 2.625894 ## sick_reg 0.9751541 0.8756257 1.087288 fit7 &lt;- glm(data = data, formula = vacc2_reg ~ covidcases, family=binomial(link=&quot;logit&quot;)) summary(fit7) ## ## Call: ## glm(formula = vacc2_reg ~ covidcases, family = binomial(link = &quot;logit&quot;), ## data = data) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 8.855e-01 2.241e-02 39.516 &lt; 2e-16 *** ## covidcases 1.185e-07 3.944e-08 3.005 0.00266 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 16039 on 13425 degrees of freedom ## Residual deviance: 16030 on 13424 degrees of freedom ## AIC: 16034 ## ## Number of Fisher Scoring iterations: 4 exp(cbind(coef(fit7), confint(fit7))) ## 2.5 % 97.5 % ## (Intercept) 2.42426 2.320307 2.533359 ## covidcases 1.00000 1.000000 1.000000 fit8 &lt;- glm(data = data, formula = vacc2_reg ~ mortalityperm, family=binomial(link=&quot;logit&quot;)) summary(fit8) ## ## Call: ## glm(formula = vacc2_reg ~ mortalityperm, family = binomial(link = &quot;logit&quot;), ## data = data) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 9.539e-01 2.766e-02 34.480 &lt;2e-16 *** ## mortalityperm -1.428e-04 8.737e-05 -1.635 0.102 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 16039 on 13425 degrees of freedom ## Residual deviance: 16036 on 13424 degrees of freedom ## AIC: 16040 ## ## Number of Fisher Scoring iterations: 4 exp(cbind(coef(fit8), confint(fit8))) ## 2.5 % 97.5 % ## (Intercept) 2.5957471 2.4591827 2.740873 ## mortalityperm 0.9998572 0.9996862 1.000029 fit9 &lt;- glm(data = data, formula = vacc2_reg ~ trusthealth, family=binomial(link=&quot;logit&quot;)) summary(fit9) ## ## Call: ## glm(formula = vacc2_reg ~ trusthealth, family = binomial(link = &quot;logit&quot;), ## data = data) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 0.58213 0.03204 18.17 &lt;2e-16 *** ## trusthealth 0.51285 0.04008 12.80 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 16039 on 13425 degrees of freedom ## Residual deviance: 15877 on 13424 degrees of freedom ## AIC: 15881 ## ## Number of Fisher Scoring iterations: 4 exp(cbind(coef(fit9), confint(fit9))) ## 2.5 % 97.5 % ## (Intercept) 1.789855 1.681205 1.906237 ## trusthealth 1.670051 1.543807 1.806430 Table tab_1 &lt;- tableone::CreateCatTable( data = data, vars = c( &quot;Gender_r&quot;, &quot;ww_fct&quot;, &quot;educ_fact&quot;, &quot;agegroup_fct&quot;, &quot;Business2&quot;, &quot;Vaccine&quot; )) tab_1 &lt;- print(tab_1) ## ## Overall ## n 13426 ## Gender_r (%) ## Female 7172 (53.5) ## Male 6129 (45.8) ## Other 94 ( 0.7) ## ww_fct (%) ## &lt;$2 per day 447 ( 3.3) ## $2-$8 per day 840 ( 6.3) ## $8-$32 per day 3011 (22.4) ## $32+ 8498 (63.3) ## Refused 630 ( 4.7) ## educ_fact (%) ## Less than high school 3830 (28.6) ## High school some college 4692 (35.0) ## Bachelor 3694 (27.6) ## Post Graduate 1179 ( 8.8) ## agegroup_fct (%) ## 18-24 2057 (15.4) ## 25-54 8360 (62.4) ## 55-64 1493 (11.1) ## 65+ 1485 (11.1) ## Business2 (%) ## 1 1179 ( 8.8) ## 2 2299 (17.1) ## 3 3488 (26.0) ## 4 4579 (34.1) ## 5 1881 (14.0) ## Vaccine (%) ## 1 1091 ( 8.1) ## 2 819 ( 6.1) ## 3 1912 (14.2) ## 4 3318 (24.7) ## 5 6286 (46.8) #write.csv(tab_1, file = &quot;table_1_vaccine-paper_basics.csv&quot;) tab_2 &lt;- tableone::CreateCatTable( data = data, strata = &quot;country_name&quot;,vars = c( &quot;Gender_r&quot;, &quot;ww_fct&quot;, &quot;educ_fact&quot;, &quot;agegroup_fct&quot;, &quot;Business2&quot;, &quot;Vaccine&quot; )) tab_2 &lt;- print(tab_2) ## Stratified by country_name ## Brazil Canada China Ecuador ## n 717 707 712 741 ## Gender_r (%) ## Female 436 (60.9) 392 (55.6) 351 (49.4) 407 (55.0) ## Male 276 (38.5) 307 (43.5) 360 (50.6) 323 (43.6) ## Other 4 ( 0.6) 6 ( 0.9) 0 ( 0.0) 10 ( 1.4) ## ww_fct (%) ## &lt;$2 per day 20 ( 2.8) 22 ( 3.1) 2 ( 0.3) 30 ( 4.0) ## $2-$8 per day 89 (12.4) 9 ( 1.3) 3 ( 0.4) 92 (12.4) ## $8-$32 per day 334 (46.6) 56 ( 7.9) 69 ( 9.7) 344 (46.4) ## $32+ 238 (33.2) 567 (80.2) 633 (88.9) 225 (30.4) ## Refused 36 ( 5.0) 53 ( 7.5) 5 ( 0.7) 50 ( 6.7) ## educ_fact (%) ## Less than high school 176 (24.6) 204 (28.9) 236 (33.2) 371 (50.1) ## High school some college 272 (38.0) 380 (53.9) 436 (61.3) 276 (37.3) ## Bachelor 232 (32.4) 97 (13.8) 33 ( 4.6) 77 (10.4) ## Post Graduate 36 ( 5.0) 24 ( 3.4) 6 ( 0.8) 16 ( 2.2) ## agegroup_fct (%) ## 18-24 171 (23.9) 102 (14.5) 85 (12.0) 224 (30.3) ## 25-54 412 (57.5) 428 (60.7) 450 (63.3) 425 (57.4) ## 55-64 57 ( 8.0) 86 (12.2) 62 ( 8.7) 58 ( 7.8) ## 65+ 76 (10.6) 89 (12.6) 114 (16.0) 33 ( 4.5) ## Business2 (%) ## 1 95 (13.2) 33 ( 4.7) 3 ( 0.4) 155 (20.9) ## 2 206 (28.7) 89 (12.6) 23 ( 3.2) 193 (26.0) ## 3 158 (22.0) 227 (32.1) 90 (12.6) 192 (25.9) ## 4 190 (26.5) 280 (39.6) 290 (40.7) 153 (20.6) ## 5 68 ( 9.5) 78 (11.0) 306 (43.0) 48 ( 6.5) ## Vaccine (%) ## 1 19 ( 2.6) 60 ( 8.5) 0 ( 0.0) 74 (10.0) ## 2 29 ( 4.0) 55 ( 7.8) 5 ( 0.7) 40 ( 5.4) ## 3 57 ( 7.9) 106 (15.0) 76 (10.7) 94 (12.7) ## 4 131 (18.3) 145 (20.5) 218 (30.6) 150 (20.2) ## 5 481 (67.1) 341 (48.2) 413 (58.0) 383 (51.7) ## Stratified by country_name ## France Germany India Italy ## n 669 722 742 736 ## Gender_r (%) ## Female 333 (49.8) 417 (58.2) 485 (66.1) 412 (56.0) ## Male 334 (49.9) 298 (41.6) 243 (33.1) 323 (43.9) ## Other 2 ( 0.3) 2 ( 0.3) 6 ( 0.8) 1 ( 0.1) ## ww_fct (%) ## &lt;$2 per day 8 ( 1.2) 2 ( 0.3) 39 ( 5.3) 4 ( 0.5) ## $2-$8 per day 5 ( 0.7) 5 ( 0.7) 172 (23.2) 2 ( 0.3) ## $8-$32 per day 48 ( 7.2) 51 ( 7.1) 360 (48.5) 44 ( 6.0) ## $32+ 561 (83.9) 634 (87.8) 163 (22.0) 651 (88.5) ## Refused 47 ( 7.0) 30 ( 4.2) 8 ( 1.1) 35 ( 4.8) ## educ_fact (%) ## Less than high school 407 (60.8) 338 (47.1) 126 (17.2) 115 (15.6) ## High school some college 134 (20.0) 133 (18.5) 429 (58.4) 378 (51.4) ## Bachelor 105 (15.7) 118 (16.5) 163 (22.2) 107 (14.5) ## Post Graduate 23 ( 3.4) 128 (17.9) 16 ( 2.2) 136 (18.5) ## agegroup_fct (%) ## 18-24 105 (15.7) 84 (11.7) 58 ( 7.9) 111 (15.1) ## 25-54 381 (57.0) 403 (56.2) 599 (81.6) 457 (62.1) ## 55-64 77 (11.5) 117 (16.3) 48 ( 6.5) 94 (12.8) ## 65+ 106 (15.8) 113 (15.8) 29 ( 4.0) 74 (10.1) ## Business2 (%) ## 1 49 ( 7.3) 32 ( 4.4) 59 ( 8.0) 46 ( 6.2) ## 2 135 (20.2) 92 (12.7) 84 (11.3) 115 (15.6) ## 3 197 (29.4) 209 (28.9) 125 (16.8) 220 (29.9) ## 4 239 (35.7) 271 (37.5) 290 (39.1) 300 (40.8) ## 5 49 ( 7.3) 118 (16.3) 184 (24.8) 55 ( 7.5) ## Vaccine (%) ## 1 74 (11.1) 74 (10.2) 42 ( 5.7) 55 ( 7.5) ## 2 49 ( 7.3) 43 ( 6.0) 42 ( 5.7) 51 ( 6.9) ## 3 152 (22.7) 111 (15.4) 105 (14.2) 109 (14.8) ## 4 192 (28.7) 167 (23.1) 224 (30.2) 198 (26.9) ## 5 202 (30.2) 327 (45.3) 329 (44.3) 323 (43.9) ## Stratified by country_name ## Mexico Nigeria Poland Russia ## n 699 670 666 680 ## Gender_r (%) ## Female 364 (52.1) 373 (55.7) 302 (45.5) 346 (50.9) ## Male 332 (47.6) 275 (41.0) 362 (54.5) 328 (48.2) ## Other 2 ( 0.3) 22 ( 3.3) 0 ( 0.0) 6 ( 0.9) ## ww_fct (%) ## &lt;$2 per day 15 ( 2.1) 195 (29.1) 13 ( 2.0) 24 ( 3.5) ## $2-$8 per day 88 (12.6) 225 (33.6) 5 ( 0.8) 42 ( 6.2) ## $8-$32 per day 306 (43.8) 175 (26.1) 316 (47.4) 340 (50.0) ## $32+ 263 (37.6) 30 ( 4.5) 286 (42.9) 256 (37.6) ## Refused 27 ( 3.9) 45 ( 6.7) 46 ( 6.9) 18 ( 2.6) ## educ_fact (%) ## Less than high school 207 (29.7) 249 (37.2) 61 ( 9.2) 67 ( 9.9) ## High school some college 413 (59.2) 325 (48.5) 295 (44.4) 157 (23.1) ## Bachelor 66 ( 9.5) 82 (12.2) 308 (46.4) 405 (59.6) ## Post Graduate 12 ( 1.7) 14 ( 2.1) 0 ( 0.0) 51 ( 7.5) ## agegroup_fct (%) ## 18-24 134 (19.2) 203 (30.3) 84 (12.7) 76 (11.2) ## 25-54 416 (59.6) 439 (65.5) 382 (57.5) 441 (64.9) ## 55-64 69 ( 9.9) 28 ( 4.2) 95 (14.3) 86 (12.6) ## 65+ 79 (11.3) 0 ( 0.0) 103 (15.5) 77 (11.3) ## Business2 (%) ## 1 85 (12.2) 86 (12.8) 78 (11.7) 94 (13.8) ## 2 146 (20.9) 130 (19.4) 134 (20.1) 184 (27.1) ## 3 157 (22.5) 158 (23.6) 161 (24.2) 218 (32.1) ## 4 233 (33.3) 200 (29.9) 215 (32.3) 147 (21.6) ## 5 78 (11.2) 96 (14.3) 78 (11.7) 37 ( 5.4) ## Vaccine (%) ## 1 54 ( 7.7) 86 (12.8) 118 (17.7) 104 (15.3) ## 2 29 ( 4.1) 53 ( 7.9) 64 ( 9.6) 75 (11.0) ## 3 83 (11.9) 94 (14.0) 109 (16.4) 128 (18.8) ## 4 138 (19.7) 127 (19.0) 134 (20.1) 192 (28.2) ## 5 395 (56.5) 310 (46.3) 241 (36.2) 181 (26.6) ## Stratified by country_name ## Singapore South Africa South Korea Spain ## n 655 619 752 748 ## Gender_r (%) ## Female 310 (47.3) 294 (47.6) 392 (52.3) 401 (53.6) ## Male 342 (52.2) 321 (51.9) 357 (47.7) 345 (46.1) ## Other 3 ( 0.5) 3 ( 0.5) 0 ( 0.0) 2 ( 0.3) ## ww_fct (%) ## &lt;$2 per day 22 ( 3.4) 9 ( 1.5) 4 ( 0.5) 5 ( 0.7) ## $2-$8 per day 37 ( 5.6) 47 ( 7.6) 4 ( 0.5) 2 ( 0.3) ## $8-$32 per day 236 (36.0) 184 (29.7) 42 ( 5.6) 29 ( 3.9) ## $32+ 339 (51.8) 329 (53.2) 678 (90.2) 692 (92.5) ## Refused 21 ( 3.2) 50 ( 8.1) 24 ( 3.2) 20 ( 2.7) ## educ_fact (%) ## Less than high school 205 (31.3) 136 (22.0) 183 (24.4) 199 (26.6) ## High school some college 219 (33.4) 85 (13.8) 145 (19.4) 129 (17.2) ## Bachelor 178 (27.2) 318 (51.5) 340 (45.4) 309 (41.3) ## Post Graduate 53 ( 8.1) 79 (12.8) 81 (10.8) 111 (14.8) ## agegroup_fct (%) ## 18-24 118 (18.0) 77 (12.5) 83 (11.1) 115 (15.4) ## 25-54 399 (60.9) 400 (64.7) 499 (66.6) 486 (65.0) ## 55-64 84 (12.8) 70 (11.3) 97 (13.0) 70 ( 9.4) ## 65+ 54 ( 8.2) 71 (11.5) 70 ( 9.3) 77 (10.3) ## Business2 (%) ## 1 40 ( 6.1) 36 ( 5.8) 20 ( 2.7) 88 (11.8) ## 2 120 (18.3) 90 (14.5) 35 ( 4.7) 173 (23.1) ## 3 153 (23.4) 211 (34.1) 161 (21.4) 178 (23.8) ## 4 245 (37.4) 208 (33.6) 362 (48.1) 234 (31.3) ## 5 97 (14.8) 74 (12.0) 174 (23.1) 75 (10.0) ## Vaccine (%) ## 1 97 (14.8) 9 ( 1.5) 11 ( 1.5) 47 ( 6.3) ## 2 40 ( 6.1) 21 ( 3.4) 27 ( 3.6) 53 ( 7.1) ## 3 73 (11.1) 84 (13.6) 114 (15.2) 92 (12.3) ## 4 126 (19.2) 189 (30.5) 273 (36.3) 156 (20.9) ## 5 319 (48.7) 316 (51.1) 327 (43.5) 400 (53.5) ## Stratified by country_name ## Sweden United Kingdom United States p ## n 650 768 773 ## Gender_r (%) &lt;0.001 ## Female 326 (50.2) 408 (53.3) 423 (55.0) ## Male 322 (49.5) 344 (44.9) 337 (43.8) ## Other 2 ( 0.3) 14 ( 1.8) 9 ( 1.2) ## ww_fct (%) &lt;0.001 ## &lt;$2 per day 18 ( 2.8) 8 ( 1.0) 7 ( 0.9) ## $2-$8 per day 1 ( 0.2) 5 ( 0.7) 7 ( 0.9) ## $8-$32 per day 2 ( 0.3) 55 ( 7.2) 20 ( 2.6) ## $32+ 593 (91.2) 644 (83.9) 716 (92.6) ## Refused 36 ( 5.5) 56 ( 7.3) 23 ( 3.0) ## educ_fact (%) &lt;0.001 ## Less than high school 325 (50.0) 167 (21.8) 58 ( 7.5) ## High school some college 146 (22.5) 197 (25.7) 143 (18.6) ## Bachelor 114 (17.5) 254 (33.2) 388 (50.5) ## Post Graduate 65 (10.0) 148 (19.3) 180 (23.4) ## agegroup_fct (%) &lt;0.001 ## 18-24 63 ( 9.7) 121 (15.8) 43 ( 5.6) ## 25-54 297 (45.7) 479 (62.5) 567 (73.7) ## 55-64 128 (19.7) 79 (10.3) 88 (11.4) ## 65+ 162 (24.9) 87 (11.4) 71 ( 9.2) ## Business2 (%) &lt;0.001 ## 1 45 ( 6.9) 67 ( 8.7) 68 ( 8.8) ## 2 121 (18.6) 104 (13.5) 125 (16.2) ## 3 257 (39.5) 234 (30.5) 182 (23.5) ## 4 162 (24.9) 271 (35.3) 289 (37.4) ## 5 65 (10.0) 92 (12.0) 109 (14.1) ## Vaccine (%) &lt;0.001 ## 1 53 ( 8.2) 60 ( 7.8) 54 ( 7.0) ## 2 56 ( 8.6) 54 ( 7.0) 33 ( 4.3) ## 3 117 (18.0) 105 (13.7) 103 (13.3) ## 4 128 (19.7) 188 (24.5) 242 (31.3) ## 5 296 (45.5) 361 (47.0) 341 (44.1) ## Stratified by country_name ## test ## n ## Gender_r (%) ## Female ## Male ## Other ## ww_fct (%) ## &lt;$2 per day ## $2-$8 per day ## $8-$32 per day ## $32+ ## Refused ## educ_fact (%) ## Less than high school ## High school some college ## Bachelor ## Post Graduate ## agegroup_fct (%) ## 18-24 ## 25-54 ## 55-64 ## 65+ ## Business2 (%) ## 1 ## 2 ## 3 ## 4 ## 5 ## Vaccine (%) ## 1 ## 2 ## 3 ## 4 ## 5 #write.csv(tab_2, file = &quot;table_2_vaccine-paper_strat-country.csv&quot;) Supplemental figure data_test &lt;- data %&gt;% group_by(country_name) %&gt;% summarise(percent_vax = round(((sum(vacc2_reg))/(length(vacc2_reg)))*100,2)) %&gt;% arrange(-percent_vax) plot &lt;- ggplot(data = data_test)+ geom_bar(aes( x = reorder(country_name, -percent_vax), y = percent_vax ), stat = &#39;identity&#39;)+ geom_text(aes(label= percent_vax, x = reorder(country_name, -percent_vax), y = percent_vax ), vjust=-1, inherit.aes = TRUE)+ theme_bw()+ theme( axis.text.x = element_text(angle = 90, size = 14), axis.text.y = element_text(size = 14), text = element_text(size = 14) )+ ylim(0,100)+ ylab(&quot;Percentage responded positive&quot;)+ xlab(&quot;Country&quot;) plot "],["id_21.html", "Bibliography", " Bibliography alex. 2022. “Answer to \"Simpler Population Pyramid in Ggplot2\".” Stack Overflow. “Babraham Bioinformatics - Trim Galore!” n.d. https://www.bioinformatics.babraham.ac.uk/projects/trim_galore/. Accessed May 12, 2024. Bobbitt, Zach. 2021. “How to Sort a Data Frame by Column in R (With Examples).” Statology. “Data on the Daily Number of New Reported COVID-19 Cases and Deaths by EU/EEA Country.” 2022. https://www.ecdc.europa.eu/en/publications-data/data-daily-new-cases-covid-19-eueea-country. DataCamp. 2020. “R Tutorial : Limitations of Linear Models.” Eeeeed. 2019. “Answer to \"Simpler Population Pyramid in Ggplot2\".” Stack Overflow. Elferts, Didzis. 2013. “Answer to \"Simpler Population Pyramid in Ggplot2\".” Stack Overflow. Gestel, Bas van, Marc Teunis, Alyanne de Haan, and Marc Teunis. n.d. Data Science Workflows Course. guyabel. 2016. “Answer to \"Simpler Population Pyramid in Ggplot2\".” Stack Overflow. Handley Wickham, and Jennifer Bryan. 2023. R Packages, 2nd Edition [Book]. 2nd ed. Louter, J. n.d. “CE.LIQ.FLOW.062_Tidydata.xlsx.” https://view.officeapps.live.com/op/view.aspx?src=https%3A%2F%2Fraw.githubusercontent.com%2FDataScienceILC%2Ftlsc-dsfb26v-20_workflows%2Fmain%2Fdata%2FCE.LIQ.FLOW.062_Tidydata.xlsx&amp;wdOrigin=BROWSELINK. Accessed May 26, 2024. Matjaz Zwitter, Milan Soklic. 1988. “Breast Cancer.” UCI Machine Learning Repository. https://doi.org/10.24432/C51P4M. Oropeza-Valdez, Juan José, Cristian Padron-Manrique, Aarón Vázquez-Jiménez, Xavier Soberon, and Osbaldo Resendis-Antonio. 2024. “Exploring Metabolic Anomalies in COVID-19 and Post-COVID-19: A Machine Learning Approach with Explainable Artificial Intelligence.” bioRxiv. https://doi.org/10.1101/2024.04.15.589583. Paganini, Julian A., Jesse J. Kerkvliet, Lisa Vader, Nienke L. Plantinga, Rodrigo Meneses, Jukka Corander, Rob J. L. Willems, Sergio Arredondo-Alonso, and Anita C. Schürch. 2024. “PlasmidEC and Gplas2: An Optimized Short-Read Approach to Predict and Reconstruct Antibiotic Resistance Plasmids in Escherichia Coli.” Microbial Genomics 10 (2): 001193. https://doi.org/10.1099/mgen.0.001193. Palayew, Adam. 2020. “COVID-19 Vaccine Paper,” August. Raschka, Sebastian. 09:00:00 +0000. “A Short Tutorial for Decent Heat Maps in R.” Sebastian Raschka, PhD. https://sebastianraschka.com/Articles/heatmaps_in_r.html. Seemann, Torsten. 2024. “Tseemann/Abricate.” Simone. 2015. “Answer to \"What Does \"Node Size\" Refer to in the Random Forest?\".” Cross Validated. Sumner, Josh, Leah Haynes, Sarah Nathan, Cynthia Hudson-Vitale, and Leslie D. McIntosh. 2020. “Reproducibility and Reporting Practices in COVID-19 Preprint Manuscripts.” medRxiv. https://doi.org/10.1101/2020.03.24.20042796. “Tidymodels - Welcome!” n.d. https://www.tidymodels.org/start/. Accessed May 26, 2024. Tomas. 2014. “Drop = TRUE Doesn’t Drop Factor Levels in Data.frame While in Vector It Does.” Forum Post. Stack Overflow. “Top 50 Ggplot2 Visualizations - The Master List (With Full R Code).” n.d. https://r-statistics.co/Top50-Ggplot2-Visualizations-MasterList-R-Code.html. Accessed May 12, 2024. “What Is Overfitting? IBM.” 2021. https://www.ibm.com/topics/overfitting. “What Is Random Forest? IBM.” 2021. https://www.ibm.com/topics/random-forest. Wick, Ryan R., Louise M. Judd, Claire L. Gorrie, and Kathryn E. Holt. 2017. “Unicycler: Resolving Bacterial Genome Assemblies from Short and Long Sequencing Reads.” PLoS Computational Biology 13 (6): e1005595. https://doi.org/10.1371/journal.pcbi.1005595. Xie, Yihui. n.d. Bookdown: Authoring Books and Technical Documents with R Markdown. Accessed May 12, 2024. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
